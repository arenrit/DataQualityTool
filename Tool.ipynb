{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessment Tool\n",
    "** Initial Development - version 1.0 **\n",
    "\n",
    "** Date: September 2017 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "\n",
    "2. [User and Beneficiary of the data quality tool](#UserType)\n",
    "\n",
    "3. [Data Quality Definitions](#Dqdef)\n",
    "\n",
    "  * [Data Completeness](#completeness)\n",
    "  * [Data Accuracy](#accuracy)\n",
    "  * [Data Uniqueness](#Unique)\n",
    "  * [Data Validity/Conformity](#Valid)\n",
    "  * [Data Integrity](#Integrity)\n",
    " \n",
    "4. [System Requirements](#Sysreq)\n",
    "\n",
    "5. [Running on Data Lake](#DataLk)\n",
    "\n",
    "6. [Preparation Phase](#PrepPhase)\n",
    "\n",
    "7. [Tool Launch](#Launch)\n",
    "\n",
    "8. [User Guide](#Uguide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**<a name=\"intro\"></a>\n",
    "\n",
    "\n",
    "DThe data quality tool has been developed to support entities in the journey to enhance the quality of their health data by providing both a baseline and insights into the areas needing improvement. Hence, this minimum viable product can be used for two purposes: \n",
    "\n",
    "1. early stage analytics projects to determine how much of the dataset can be used moving forward\n",
    "2. when putting in place a data quality improvement roadmap to determine the baseline, identify the gaps and the required actions\n",
    "\n",
    "The quality tool has been built around the minimum health data set, a collection of 80 data elements available in the customer policy, medical claim and, data related to distributors or providers. The minimum health dataset was approved by the Health Data Experts Community, under the coordination of the Global Line health and is the minimum required for every entity to collect and store, regardless of level of maturity, to perform insurance (e.g. pricing, underwriting, product development) and non-insurance related analytics, (e.g. provider profiling, population risk adjustments, Fraud Waste and Aabuse)\n",
    "\n",
    "However, the tool has been developed to work with any kind of structured data,  - not necessarily part of minimum health dataset, making is reusable for other business lines and functions. \n",
    "The tool will check data quality against 5 dimensions: completeness, integrity, accuracy, uniqueness and, validity / conformity and will generate a series of scores:\n",
    "\n",
    "*  At dataset level across dimensions \n",
    "*  At dataset level per dimension \n",
    "*  At variable level across dimensions\n",
    "*  At variable level per dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User and Beneficiary of the data quality tool**<a name=\"UserType\"></a>\n",
    "\n",
    "The tool can be used by anyone with either no or basic Python skills, working in the Data Office, Line of Business or any function where the user is expected to work with health data (clinical and non-clinical). To further develop or change the tool, the user would need basic to medium Python skills.\n",
    "\n",
    "The user will play a key role in the Preparation Phase, where he / she will be expected to map the local data against the minimum health data set and adjust rules based on the business requirements. This person will also be responsible for running the tool and supporting the beneficiary in interpreting the results. \n",
    "\n",
    "The beneficiary of the tool is a business owner who is either leading an analytics project and needs to understand the confidence level in the dataset before building the analytics model, or is in charge of maintaining and improving quality of given data elements as part of their data steward role. The beneficiary will support the user in identifying the right data to assess, adjusting the rules and putting in place the actions to improve quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Quality Definitions** <a name=\"Dqdef\"></a>\n",
    "\n",
    "Data Quality in the current tool is assessed on following 5 dimensions - \n",
    "\n",
    "* **Data Completeness**<a name = \"completeness\"></a>\n",
    "\n",
    "    _Definition:_ It is defined as expected comprehensiveness.\n",
    "    \n",
    "    _Scope of data:_ Applicable to all variables (approx. 80) in the minimum health data set, with few exceptions (e.g. beneficiary related data – name, date of birth / death etc.). Exceptions are provided in Excel document.\n",
    "    \n",
    "    _Calculation method:_ Calculate fill rates for each variable, the measure of the blank (null or empty string) values or the presence of non-blank values. For mandatory data items, for example – claim ID, policy start date etc., 100% completeness is required. \n",
    "    \n",
    "    _Unit of measure:_ Percentage records. This will be calculate after applying any validity conditions, e.g. if Hospitalization Date is to be present only for in-patients, the numerator will be # in-patients records where Hospitalization Date is populated, and denominator will be total # in-patient records\n",
    "    \n",
    "____________________________________________________________________________________________________________\n",
    "    \n",
    "* **Data Accuracy**<a name = \"accuracy\"></a>\n",
    "\n",
    "    _Definition:_ It is defined as the degree to which data correctly reflects in terms of **form** and **content** the real-world object or an event being described.\n",
    "    \n",
    "    To check accuracy of form it is required for each variable to have a predefined format. Checking content accuracy is more complex and it can be done by manually checking that the content reflects the real world (impossible with large datasets), benchmarking against other data sources internal or external, or by inferring from other variables in the data set what the variable under discussion could be in the real world.\n",
    "    \n",
    "    Hence, the focus will be more on format and less on content accuracy as this would require having additional data sources. \n",
    "    \n",
    "    _Scope of Data:_ Applicable to all variables in the minimum health data set that can have a predefined format to check accuracy against. \n",
    "    \n",
    "    _Calculation method:_ For each variable, determine the compliance with a given format (Yes if compliant / No if not compliant).\n",
    "    \n",
    "    Calculate for the entire data set the percentage of compliance\n",
    "    \n",
    "    _Unit of measure :_ Percentage records\n",
    "    \n",
    "____________________________________________________________________________________________________________\n",
    "    \n",
    "* **Data Uniqueness**<a name = \"Unique\"></a>\n",
    "    \n",
    "    _Definition:_ Refers to the fact that a variable flagged as having to be unique is recorded only once, otherwise it is a duplicate.\n",
    "    \n",
    "    **YES** example: Member ID is unique as it correspondents to one client only.\n",
    "    \n",
    "    **No** example: date of birth is not a unique value as people can be born on the same day. \n",
    "    \n",
    "    _Scope of data:_ Applicable to all variables within the minimum health data set, but relevant only for those variables flagged as unique.\n",
    "    \n",
    "    The variables flagged as unique in the minimum health data set are: ID Claim, ID Policy, ID Member, ID distributor and ID provider\n",
    "    \n",
    "    _Calculation method:_ Calculate if there are any observations which do not have any differentiation in values \n",
    "    \n",
    "    _Unit of measure:_ Percentage Records\n",
    "    \n",
    "____________________________________________________________________________________________________________\n",
    "    \n",
    "* **Data Validity/Conformity**<a name = \"Valid\"></a>\n",
    "\n",
    "    _Definition:_ A variable passes the validity check if it complies with type, size and format. \n",
    "    \n",
    "    _Scope of data:_ Applicable to all variables (approx. 80) in the minimum health data set that:\n",
    "    \n",
    "        A.\tconform to a given range, in case of numeric, or \n",
    "        B.\thave correct spelling and is discoverable, in case of character variables.\n",
    "        \n",
    "    Examples:\n",
    "    \n",
    "        1.\tAge of insured is between 5 and 95.\n",
    "        2.\tPolicy start date is after 1/1/1999.\n",
    "    \n",
    "    _Calculation method:_ Comparison of variable values to standard type, format or range.\n",
    "    \n",
    "    _Pre-requisite:_ define for each variable whether the check will be done against type / format / range / all and define the associated standards.\n",
    "    \n",
    "    _Unit of measure:_ Percentage Records\n",
    "    \n",
    "____________________________________________________________________________________________________________\n",
    "    \n",
    "* **Data Integrity**<a name = \"Integrity\"></a>\n",
    "\n",
    "    _Definition:_ It is defined as a measure of variable values corresponding to set of rules, natural or defined by business. For example, date of birth of insured is same as or greater than policy start date or claim registration date is earlier or same as claim payment date.\n",
    "    \n",
    "    _Scope of data:_ Applicable to most variables (approx. 80) in the minimum health data set except for standalone variables (e.g. name of policy holder)\n",
    "    \n",
    "    _Calculation method:_ Comparison of data format with metadata or data documentation (pre-requisite to have from entity)\n",
    "    \n",
    "    _Unit of measure:_ Percentage records\n",
    "    \n",
    "____________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**System Requirements**<a name = \"Sysreq\"></a>\n",
    "\n",
    "Tool has been developed on Python, and visualization layer has been built on Bokeh javascripts with its native bindings in Python. In current development version, the tool runs locally on the Desktop/Computer, and takes input from Excel files.\n",
    "\n",
    "To run the tool, user must have Python 2.7 or above (Current development supports v 3.4), with following python packages\n",
    "\n",
    "1. Tkinter\n",
    "2. openpyxl\n",
    "3. pandas\n",
    "4. numpy\n",
    "5. re\n",
    "6. datetime\n",
    "7. math\n",
    "8. bokeh\n",
    "\n",
    "Please get the latest vesion of all the packages from pip. All of these packages are part of [Anaconda Python installation](https://www.anaconda.com/download/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running on Data Lake**<a name = \"DataLk\"></a>\n",
    "\n",
    "\n",
    "The tool uses native Python libraries, can be used on any machine including Data Lakes.\n",
    "\n",
    "In case, the tool needs to be run on the datalake where we don't have UI, all the files and locations can be provided using XML files. A file named _Config.xml_ should be placed in the same directory as the code. This XML file provides the location of Mapping XML, Rule Description and Input Data. If this file is present, tool will not open the dialogue boxes to ask file locations, rather it will automatically read the files.\n",
    "\n",
    "Bokeh will push the result on a particular port - generally 5006, but it can also push on other ports, and will display on which port it has pushed on the terminal. How the page will be viewed will depend on the server settings, but in general, visualization will be available on _server address:5006_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation Phase** <a name = \"PrepPhase\"></a>\n",
    "\n",
    "The preparation phase requires creation of two files : \n",
    "\n",
    "* _Variable Mapping:_ The mapping of the variable needs to be provided in an excel file. This is the mapping of variable names (without spaces and with underscores) to more readable and meaningful variable names. Though the tool aims to be much flexible and incorporate any variable if mapping is provided, current focus is on developing mapping against the minimum dataset prepared by AGL team. \n",
    "\n",
    "This is to ensure that different variable names in different entities can be incorporated, e.g. in an entitiy 'Date of Birth' can be named as 'BirthDate' and 'Policy Number' as 'ContractNumber' the current workbook is formatted in a way that it can take data from multiple tables from a single database, or multiple sheets from an excel workbook.\n",
    "    \n",
    "____________________________________________________________________________________________________________\n",
    "\n",
    "* _Rule Mapping:_ In an excel file, various rules with different weights needs to be provided for the evaluation of data. This rule mapping will be entity specific, as Accuracy, Validity and Integirty rules will be entity dependent, e.g. in UK a real claim amount could be between 1000, 100K in GBP, but in Japan, that will be much higher number in Yen. Similarly, Date and medical coding standards may be different across countries.\n",
    "\n",
    "    Currently to provide a complete description of a rule, 6 fields needs to be provided:\n",
    "\n",
    "    * _Rule Id:_ This is just a unique ID to identify the rule. However the sorting of the rules for a particular variable is important as sorting will determine the type of visualization for the rule\n",
    "    \n",
    "    * _Dimension:_ This rule is for which of the five dimension explained above\n",
    "    \n",
    "    * _Weight:_ Weight of the rule, in terms of importance, in the evaluation of overall data quality across the dimension\n",
    "    \n",
    "    * _Depvar:_ Depvar is a variable, on which the current variable and rule is dependent, e.g. Admission Date should be populated(completeness rule), when Admit Type is in-patient. In this case, 'Admit Type' is Depvar. \n",
    "    \n",
    "    Please note that the human readable variable name must be provided. The tool will automatically fetch the mapping from Variable Mapping\n",
    "    \n",
    "    * _Condition:_ This condition works on the Dependent variable, and is a way to provide the functionality to read the inequality/equality/list condition. \n",
    "    \n",
    "    In current scope of development, this can take 5 values\n",
    "    \n",
    "        1. **>=**: Evaluate rule when Depvar >= the criterion (described below)\n",
    "        2. **<=**: Evaluate rule when Depvar <= the criterion (described below)\n",
    "        3. **>**: Evaluate rule when Depvar > the criterion (described below)\n",
    "        4. **<**: Evaluate rule when Depvar < the criterion (described below)\n",
    "        5. **in**: Evaluate rule when Depvar in the criterion - A list of values(described below)\n",
    "        \n",
    "    * _Criterion:_ This criterion is to compare the Depvar. The comparison function is provided by **Condition** as described above. Depvar is the left hand side of the comparison, and this criterion is right hand side of the comparison equation/inequation. \n",
    "    \n",
    "    Criterion can take following values:\n",
    "    \n",
    "        1. **Single Value**: Compare with the single value. In case of list comparison, one element list will be used\n",
    "        2. **List**: Used for list comparison. List to be provided in parenthesis, and separated by \",\". For list of String/Characters, use single quotes around those, e.g. ('A', 'B', 'C')\n",
    "        3. **this**: Variable on which rule is being evaluated, will be used on right hand side, e.g. if a rule is being evaluated for Amount Paid, and integrity condition is Amount Billed > Amount Paid, then Depvar will be Amount Billed, Condition will be '>' and Criterion will be 'this'\n",
    "    \n",
    "    * _C1 :_ This is similar to condition, but this condition will be applied on the variable itself. This will be genetally used in validity and accuracy rules.\n",
    "    \n",
    "    C1 can take 3 values:\n",
    "  \n",
    "        1. **Format** : To check the format of variables - Number, Character, Date format (in DDMMYYYY or similar format), or a regex. More details of format in Criterion1 below.\n",
    "        2. **Range** : Currently supports closed form ranges for numbers and Dates. Range values will be provided in Criterion1 variable. Numbered Range to be provided in _[Lower End, Upper End]_ format. Date will be in similar format, but the date should be written in DD/MM/YYYY format only\n",
    "        3. **Values** : A list of values to be compared against. The list of values will be provided in Criterion 1 Variable. List to be provided in paranthesis, and separated by \",\". For list of String/Characters, use single quotes around those, e.g. ('A', 'B', 'C')\n",
    "        \n",
    "    * _Criterion1:_ Criterion1 to be provided for the checking the conditions on this variable (as discussed in **C1** above). Criterion1 for Range and Values has already been detailed out. \n",
    "    \n",
    "    For **C1=Format**, criterion1 can take following values - \n",
    "    \n",
    "        1. **Character**: To check if variable is character\n",
    "        2. **Number**: To check if variable is numeric\n",
    "        3. **Date Format**: Date format to be provided in common language format e.g. DDMMYYYY or MMDDYYYY etc.\n",
    "        4. **Regex**: Value will be 'Regex::{Regular Expression to evaluate}'. This will check if a particular field follows a particular type of format described by regex.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tool Launch ** <a name = \"Launch\"></a>\n",
    "\n",
    "Following are the steps to run the tool on the windows PC. For MacOS/Unix, please use the respective terminals.\n",
    "\n",
    "1. Open command prompt in windows. In case of Unix, use the unix terminal\n",
    "2. Go to the directory where you have saved 'Tool.py' file. Use cd /D \"<\\Directory Path\\>\". Don't use /D in case of Unix\n",
    "3. Once you are in the directory, use \"bokeh serve --show tool.py\" to run the tool\n",
    "\n",
    "Once the tool is launched, a few small dialog boxes will appear to ask the location of Variable Mapping Excel (Current version only supports excel), Type of Data (Please select Excel as of now, as current version only supports excel), and then it will ask for the data file and rule description file. \n",
    "\n",
    "In case, you are running tool on a server with SSH connection without UI, you will have to use _Config.xml_, and the dialog boxes will not open. To run the tool on server, use command \n",
    "\n",
    "`nohup bokeh serve Tool.py --allow-websocket-origin [Host:port] & `\n",
    "\n",
    "where [Host:port] is public IP on which all TCP connections are allowed (or IP on which server will listen to connections from particular ports). To access tool, use [Host:port] from any browser on allowed network.\n",
    "\n",
    "Once all the inputs are provided, the tool will take some 1-2 minutes to calculate at the backend, and then will open the GUI on the browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** User Guide ** <a name = \"Uguide\"></a>\n",
    "\n",
    "Once the bokeh server is up and running, tool can be opened in a browser, with allowed Host and Port. On the opened page, two tabs are available as shown in the image:\n",
    "\n",
    "![Tool Landing Page](MainPage.JPG \"Tool Main Page\")\n",
    "\n",
    "On the Summary tab, you see the oevrall score and a table with dimension-wise score. On clicking the table, the description of the dimension will come at bottom half pf the page\n",
    "\n",
    "In the middle of the page, a download button is available to download erroneous data.\n",
    "\n",
    "On selecting detailed tab, you will see 4 major components:\n",
    "\n",
    "1. _Dropdown on left top_ : This dropdown provides how the table on left is summarized - by a particular dimension or across the dimension\n",
    "2. _Table on left bottom (almost entire left)_ : This table is the variable lavel score by the dimensions specified by the dimension as defined in 1. This table works as selection. Select any variable for which you want to see the information on the right.\n",
    "3. _Chart on the right top_ : The chart tries to show the distribution of the variable with certain rules and criterion. Please note the variable description on the top\n",
    "4. _Table on bottom right_ : Variable and dimension level scores. On hover, also gives the rule description.\n",
    "\n",
    "![Detail Page](changes.JPG \"Detail page\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Code\n",
    "<a name = \"Code\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required packages**<a name=\"packageImport\"></a>\n",
    "\n",
    "A significant amount of packages are required. All the packages are available in Anaconda Distribution. Please ensure that Bokeh is updated to 0.12.6. Bokeh update will also update numpy to required level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Minimum Data Quality Tool\n",
    "\n",
    "This tools is created to generate visual reports to inspect the data quality of given data against a few rules.\n",
    "Rules are to be defined in a ceratin format as explained in markdown documents.\n",
    "\n",
    "Tool extensively usage Python Data Analysis capabilities including pandas, numpy, UI from Bokeh and Python Tkinter both.\n",
    "\n",
    "Current version works with python 2.7 and 3.4 both\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '1.0'\n",
    "__author__ = 'Ritesh Agrawal'\n",
    "\n",
    "import Tkinter, tkFileDialog\n",
    "from Tkinter import *\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from math import pi\n",
    "from bokeh.models.ranges import FactorRange, Range1d\n",
    "from bokeh.io import output_file, show, output_notebook, push_notebook\n",
    "from bokeh.layouts import widgetbox\n",
    "from bokeh.models.widgets import Select, Panel, Tabs, DataTable, DateFormatter, TableColumn, HTMLTemplateFormatter, Div, Button\n",
    "from bokeh.layouts import column, widgetbox, row\n",
    "from bokeh.plotting import figure, curdoc\n",
    "from bokeh.models import ColumnDataSource, Range1d, Legend, CustomJS\n",
    "from bokeh.models.annotations import Label as lb\n",
    "from bokeh.charts import Bar, defaults\n",
    "from bokeh.charts.attributes import CatAttr\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Chart Colors** <a name=\"colors\"></a>\n",
    "\n",
    "This section defines the colors used in charts. Please chnage the color pallate as per you taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = ['salmon', 'lightgreen', 'DeepSkyBlue', 'FireBrick', 'DimGray', 'ForestGreen', 'Fuchsia', 'MediumSlateBlue ', \\\n",
    "          'HotPink', 'MediumSpringGreen', 'SeaGreen', 'DarkRed', 'MediumPurple', 'Sienna', 'LightSteelBlue', 'OliveDrab', \\\n",
    "         'Maroon', 'Olive', 'Purple', 'YellowGreen']\n",
    "defaultcolor = 'SlateBlue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UI Class for Mapping Selection**<a name=\"MappingUI\"></a>\n",
    "\n",
    "A class extending Tkinter, to create a small dialogue box for user to select the mapping file. Class first create a window to prompt user to browse to mapping file. Browsing is also done based on a UI file picker. Once user clicks the 'Load' button, excel file is read as pandas dataframe and is made global to be used outside of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Class extends tkinter\n",
    "class Mappingpick(Tkinter.Tk):\n",
    "    \"\"\" Class to generate UI for Mapping File Selection\n",
    "    \n",
    "    Class use tkinter to generate UI which allows user to select an excel file\n",
    "    which contains mapping of variables\n",
    "    \"\"\"\n",
    "    \n",
    "    def getPath(self):\n",
    "        \"\"\" Method to select File\n",
    "        \n",
    "        Method used a tkFileDialog (File chooser DialogBox) to allow user to select file with Mapping\n",
    "        Selected File path is returned back to tkinter window\n",
    "        \"\"\"\n",
    "        ftypes = [('ExcelFiles', '*.xls;*.xlsx;*.xlsm'),] #List of excel compatible file formats\n",
    "        f = tkFileDialog.askopenfilename(filetypes=ftypes) # Creating a file chooser dialog box with only excel files\n",
    "        self.E1.insert(0, f) #Putting the selected file name with entire path in Tkinter entry element\n",
    "        \n",
    "    def loadFile(self):\n",
    "        \"\"\"Method to read file\n",
    "        \n",
    "        Method reads excel file as Pandas DataFrame. \n",
    "        This Dataframe is stored as global variable to be used outside class\n",
    "        \"\"\"\n",
    "        global mapping # defining global variable\n",
    "        mapping = pd.read_excel(self.E1.get()) #Reading excel file and assigning global variable reference to read dataframe\n",
    "        self.destroy() #destroying the tkinter window\n",
    "    \n",
    "    def __init__(self, parent):\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        Creates an empty tkinter window. Window is not resizable\n",
    "        \"\"\"\n",
    "        Tkinter.Tk.__init__(self,parent) #initializing tkinter window\n",
    "        self.resizable(False, False) # make window non-resizable\n",
    "        self.parent=parent #Get the properties of tkinter\n",
    "        self.putelements() #draw the elements on window\n",
    "    \n",
    "    def putelements(self):\n",
    "        \"\"\"Method to draw UI elements \n",
    "        \n",
    "        Method puts various UI elements, e.g. Label, buttons etc. on tkinter window\n",
    "        Method also calls a file picker to select the file\n",
    "        \"\"\"\n",
    "        self.grid() #use grid layout manager\n",
    "        self.fr1 = Tkinter.Frame(self) #create a new frame on tkinter window\n",
    "        self.fr1.grid(column=0,row=0,sticky='EW') #add frame to grid\n",
    "        self.fr2 = Tkinter.Frame(self) # adding another frame\n",
    "        self.fr2.grid(column=0,row=1,sticky='EW') #add frame to grid\n",
    "        self.fr3 = Tkinter.Frame(self) #add another frame in row\n",
    "        self.fr3.grid(column=0,row=2,sticky='EW') #add frame to grid\n",
    "        self.L1 = Tkinter.Label(self.fr1, text=\"Please provide the Variable Mapping\") #add a label on top frame\n",
    "        self.L1.grid(row=0, column=0, sticky='EW') #add label to grid\n",
    "        self.L2 = Tkinter.Label(self.fr2, text=\"FilePath\")#create a label in second row frame\n",
    "        self.L2.grid(row=0, column=0, sticky='EW')#add label to grid in left-most column\n",
    "        self.E1 = Tkinter.Entry(self.fr2, bd =5)#create an entry box in second row frame\n",
    "        self.E1.grid(row=0, column=1, sticky='EW')#add entry box in middle column\n",
    "        #create a button in second row frame. button to open a file chooser dialog box for file selection \n",
    "        self.B1 = Tkinter.Button(self.fr2, text =\"Browse\", command = self.getPath)\n",
    "        self.B1.grid(row=0, column=2, sticky='EW')#adding button to last column\n",
    "        #create a button in third row frame. Button to read excel file defined in entry as pandas dataframe\n",
    "        self.B2 = Tkinter.Button(self.fr3, text =\"Load\", command = self.loadFile)\n",
    "        self.B2.grid(row=0, column=0, sticky='EW')#add button to last row frame\n",
    "        #assign weights to rows and columns for width and height adjustment\n",
    "        self.grid_columnconfigure(0,weight=1)\n",
    "        self.grid_rowconfigure(0,weight=1)\n",
    "        self.grid_rowconfigure(1,weight=1)\n",
    "        self.fr1.grid_columnconfigure(0,weight=1)\n",
    "        self.fr1.grid_rowconfigure(0,weight=1)\n",
    "        self.fr3.grid_columnconfigure(0,weight=1)\n",
    "        self.fr3.grid_rowconfigure(0,weight=1)\n",
    "        self.fr2.grid_columnconfigure(0,weight=1)\n",
    "        self.fr2.grid_columnconfigure(1,weight=1)\n",
    "        self.fr2.grid_columnconfigure(2,weight=1)\n",
    "        self.fr2.grid_rowconfigure(0,weight=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UI class for Data Selection**<a name=\"DataUI\"></a>\n",
    "\n",
    "A class extending Tkinter, to create a small dialogue box for user to select the Data. Class first create a window to prompt user to browse to mapping file. Browsing is also done based on a UI file picker. Once user clicks the 'Load' button, different sheets of excel file is read as pandas dataframe and stored as dictionary of dataframes. Dictionary is made global to be used outside of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class extends tkinter\n",
    "class Datapick(Tkinter.Tk):\n",
    "    \"\"\" Class to generate UI for Input Data File Selection\n",
    "    \n",
    "    Class use tkinter to generate UI which allows user to select an excel file\n",
    "    which contains the actual data in various sheets\n",
    "    \"\"\"\n",
    "    \n",
    "    def getPath(self):\n",
    "        \"\"\" Method to select File\n",
    "        \n",
    "        Method used a tkFileDialog (File chooser DialogBox) to allow user to select file with Data\n",
    "        Selected File path is returned back to tkinter window\n",
    "        \"\"\"\n",
    "        ftypes = [('ExcelFiles', '*.xls;*.xlsx;*.xlsm'),] #List of excel compatible file formats\n",
    "        f = tkFileDialog.askopenfilename(filetypes=ftypes)# Creating a file chooser dialog box with only excel files\n",
    "        self.E1.insert(0, f)#Putting the selected file name with entire path in Tkinter entry element\n",
    "        \n",
    "    def loadFile(self):\n",
    "        \"\"\"Method to read file\n",
    "        \n",
    "        Method reads various tables from excel file as dictionary of Pandas DataFrame. \n",
    "        This dictionary is stored as global variable to be used outside class\n",
    "        \"\"\"\n",
    "        global entire_data #defining a variable global\n",
    "        wb = load_workbook(filename = self.E1.get(), read_only=True) #loading excel workbook to fetch sheetnames\n",
    "        shtlst = wb.sheetnames #get the names of excel sheets\n",
    "        entire_data = {} #create empty dictionary and assign to global variable\n",
    "        for i in range(len(shtlst)):\n",
    "            entire_data[shtlst[i]] = pd.read_excel(self.E1.get(), sheetname=shtlst[i]) #iteratively read all the worrksheets and save in dict\n",
    "        self.destroy() #close tkinter window\n",
    "        \n",
    "    \n",
    "    def __init__(self, parent):\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        Creates an empty tkinter window. Window is not resizable\n",
    "        \"\"\"\n",
    "        Tkinter.Tk.__init__(self,parent) #initializing tkinter window\n",
    "        self.resizable(False, False) # make window non-resizable\n",
    "        self.parent=parent #Get the properties of tkinter\n",
    "        self.putelements() #draw the elements on window\n",
    "    \n",
    "    def putelements(self):\n",
    "        \"\"\"Method to draw UI elements \n",
    "        \n",
    "        Method puts various UI elements, e.g. Label, buttons etc. on tkinter window\n",
    "        Method also calls a file picker to select the file\n",
    "        \"\"\"\n",
    "        self.grid() #use grid layout manager\n",
    "        self.fr1 = Tkinter.Frame(self) #create a new frame on tkinter window\n",
    "        self.fr1.grid(column=0,row=0,sticky='EW') #add frame to grid\n",
    "        self.fr2 = Tkinter.Frame(self) # adding another frame\n",
    "        self.fr2.grid(column=0,row=1,sticky='EW') #add frame to grid\n",
    "        self.fr3 = Tkinter.Frame(self) #add another frame in row\n",
    "        self.fr3.grid(column=0,row=2,sticky='EW') #add frame to grid\n",
    "        self.L1 = Tkinter.Label(self.fr1, text=\"Please provide the Input Data\") #add a label on top frame\n",
    "        self.L1.grid(row=0, column=0, sticky='EW') #add label to grid\n",
    "        self.L2 = Tkinter.Label(self.fr2, text=\"FilePath\")#create a label in second row frame\n",
    "        self.L2.grid(row=0, column=0, sticky='EW')#add label to grid in left-most column\n",
    "        self.E1 = Tkinter.Entry(self.fr2, bd =5)#create an entry box in second row frame\n",
    "        self.E1.grid(row=0, column=1, sticky='EW')#add entry box in middle column\n",
    "        #create a button in second row frame. button to open a file chooser dialog box for file selection \n",
    "        self.B1 = Tkinter.Button(self.fr2, text =\"Browse\", command = self.getPath)\n",
    "        self.B1.grid(row=0, column=2, sticky='EW')#adding button to last column\n",
    "        #create a button in third row frame. Button to read excel file defined in entry as pandas dataframe\n",
    "        self.B2 = Tkinter.Button(self.fr3, text =\"Load\", command = self.loadFile)\n",
    "        self.B2.grid(row=0, column=0, sticky='EW')#add button to last row frame\n",
    "        #assign weights to rows and columns for width and height adjustment\n",
    "        self.grid_columnconfigure(0,weight=1)\n",
    "        self.grid_rowconfigure(0,weight=1)\n",
    "        self.grid_rowconfigure(1,weight=1)\n",
    "        self.fr1.grid_columnconfigure(0,weight=1)\n",
    "        self.fr1.grid_rowconfigure(0,weight=1)\n",
    "        self.fr3.grid_columnconfigure(0,weight=1)\n",
    "        self.fr3.grid_rowconfigure(0,weight=1)\n",
    "        self.fr2.grid_columnconfigure(0,weight=1)\n",
    "        self.fr2.grid_columnconfigure(1,weight=1)\n",
    "        self.fr2.grid_columnconfigure(2,weight=1)\n",
    "        self.fr2.grid_rowconfigure(0,weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UI Class for Rules Selection**<a name=\"RulesUI\"></a>\n",
    "\n",
    "A class extending Tkinter, to create a small dialogue box for user to select the Rules file. Class first create a window to prompt user to browse to mapping file. Browsing is also done based on a UI file picker. Once user clicks the 'Load' button, excel file is read as pandas dataframe and is made global to be used outside of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class extends tkinter - Python native UI class'\n",
    "class Rulespick(Tkinter.Tk):\n",
    "    \"\"\" Class to generate UI for Input Data File Selection\n",
    "    \n",
    "    Class use tkinter to generate UI which allows user to select an excel file\n",
    "    which contains the actual data in various sheets\n",
    "    \"\"\"\n",
    "    \n",
    "    def getPath(self):\n",
    "        \"\"\" Method to select File\n",
    "        \n",
    "        Method used a tkFileDialog (File chooser DialogBox) to allow user to select file with Rules Description\n",
    "        Selected File path is returned back to tkinter window\n",
    "        \"\"\"\n",
    "        ftypes = [('ExcelFiles', '*.xls;*.xlsx;*.xlsm'),]#List of excel compatible file formats\n",
    "        f = tkFileDialog.askopenfilename(filetypes=ftypes)# Creating a file chooser dialog box with only excel files\n",
    "        self.E1.insert(0, f)#Putting the selected file name with entire path in Tkinter entry element\n",
    "        \n",
    "    def loadFile(self):\n",
    "        global rulelist #defining a variable global\n",
    "        rulelist = pd.read_excel(self.E1.get())#Reading excel file and assigning global variable reference to read dataframe\n",
    "        self.destroy()#close tkinter window\n",
    "    \n",
    "    def __init__(self, parent):\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        Creates an empty tkinter window. Window is not resizable\n",
    "        \"\"\"\n",
    "        Tkinter.Tk.__init__(self,parent) #initializing tkinter window\n",
    "        self.resizable(False, False) # make window non-resizable\n",
    "        self.parent=parent #Get the properties of tkinter\n",
    "        self.putelements() #draw the elements on window\n",
    "    \n",
    "    def putelements(self):\n",
    "        \"\"\"Method to draw UI elements \n",
    "        \n",
    "        Method puts various UI elements, e.g. Label, buttons etc. on tkinter window\n",
    "        Method also calls a file picker to select the file\n",
    "        \"\"\"\n",
    "        self.grid() #use grid layout manager\n",
    "        self.fr1 = Tkinter.Frame(self) #create a new frame on tkinter window\n",
    "        self.fr1.grid(column=0,row=0,sticky='EW') #add frame to grid\n",
    "        self.fr2 = Tkinter.Frame(self) # adding another frame\n",
    "        self.fr2.grid(column=0,row=1,sticky='EW') #add frame to grid\n",
    "        self.fr3 = Tkinter.Frame(self) #add another frame in row\n",
    "        self.fr3.grid(column=0,row=2,sticky='EW') #add frame to grid\n",
    "        self.L1 = Tkinter.Label(self.fr1, text=\"Please provide the Rules Description\") #add a label on top frame\n",
    "        self.L1.grid(row=0, column=0, sticky='EW') #add label to grid\n",
    "        self.L2 = Tkinter.Label(self.fr2, text=\"FilePath\")#create a label in second row frame\n",
    "        self.L2.grid(row=0, column=0, sticky='EW')#add label to grid in left-most column\n",
    "        self.E1 = Tkinter.Entry(self.fr2, bd =5)#create an entry box in second row frame\n",
    "        self.E1.grid(row=0, column=1, sticky='EW')#add entry box in middle column\n",
    "        #create a button in second row frame. button to open a file chooser dialog box for file selection \n",
    "        self.B1 = Tkinter.Button(self.fr2, text =\"Browse\", command = self.getPath)\n",
    "        self.B1.grid(row=0, column=2, sticky='EW')#adding button to last column\n",
    "        #create a button in third row frame. Button to read excel file defined in entry as pandas dataframe\n",
    "        self.B2 = Tkinter.Button(self.fr3, text =\"Load\", command = self.loadFile)\n",
    "        self.B2.grid(row=0, column=0, sticky='EW')#add button to last row frame\n",
    "        #assign weights to rows and columns for width and height adjustment\n",
    "        self.grid_columnconfigure(0,weight=1)\n",
    "        self.grid_rowconfigure(0,weight=1)\n",
    "        self.grid_rowconfigure(1,weight=1)\n",
    "        self.fr1.grid_columnconfigure(0,weight=1)\n",
    "        self.fr1.grid_rowconfigure(0,weight=1)\n",
    "        self.fr3.grid_columnconfigure(0,weight=1)\n",
    "        self.fr3.grid_rowconfigure(0,weight=1)\n",
    "        self.fr2.grid_columnconfigure(0,weight=1)\n",
    "        self.fr2.grid_columnconfigure(1,weight=1)\n",
    "        self.fr2.grid_columnconfigure(2,weight=1)\n",
    "        self.fr2.grid_rowconfigure(0,weight=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UI Class for DataType Selection**<a name=\"TypeUI\"></a>\n",
    "\n",
    "A class extending Tkinter, to create a small dialogue box for user to select the DataType. Class first create a window to prompt user to select type of data out of 'Database', 'csv', or 'Excel', using Radiobuttons (so that user can select only one of those). \n",
    "\n",
    "<span style=\"color:red\"><b>Please do note that current version only supports input data in excel format</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class extends tkinter - Python native UI library \n",
    "class Typepick(Tkinter.Tk):\n",
    "    \"\"\" Class to generate UI for Input Data Type Selection\n",
    "    \n",
    "    Class use tkinter to generate UI which allows user to select type of file\n",
    "    User can choose among 'Database', 'csv' and 'Excel'. \n",
    "    Radio buttons are used, so that user can pick only single type of file\n",
    "    \"\"\" \n",
    "    \n",
    "    def sel(self):\n",
    "        \"\"\"Method to get the input type from user selection\n",
    "        \n",
    "        Stores the user selection in a global variable and closes the UI\n",
    "        \"\"\"\n",
    "        global InputType #declare a variable global\n",
    "        InputType= self.var.get() #assign the global variable value of user selection\n",
    "        self.destroy() #close tkinter window\n",
    "\n",
    "    def __init__(self, parent):\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        Creates an empty tkinter window. Window is not resizable\n",
    "        \"\"\"\n",
    "        Tkinter.Tk.__init__(self,parent) #initializing tkinter window\n",
    "        self.resizable(False, False) # make window non-resizable\n",
    "        self.parent=parent #Get the properties of tkinter\n",
    "        self.putelements() #draw the elements on window\n",
    "    \n",
    "    def putelements(self):\n",
    "        \"\"\"Method to draw UI elements \n",
    "        \n",
    "        Method puts Radio buttons to select the fie type. Method uses pack layout manager\n",
    "        \"\"\"\n",
    "        self.label = Label(self) #create a lable\n",
    "        self.label.config(text = \"Please choose input source\") #add text to label\n",
    "        self.label.pack() #pack the label\n",
    "        self.var = IntVar() #create a variable to get the value from radio buttons\n",
    "        self.R1 = Radiobutton(self, text=\"CSV\", variable=self.var, value=1, command=self.sel) #create a radio button for csv\n",
    "        self.R1.pack( anchor = W ) #add radio button to UI\n",
    "        self.R2 = Radiobutton(self, text=\"DataBase\", variable=self.var, value=2, command=self.sel)#create a radio button for db\n",
    "        self.R2.pack( anchor = W ) #add radio button to UI\n",
    "        self.R3 = Radiobutton(self, text=\"Excel\", variable=self.var, value=3, command=self.sel)#create a radio button for excel\n",
    "        self.R3.pack(anchor = W) #add radio button to UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to check if XML file is provided** <a name=\"XMLFunc\"></a>\n",
    "\n",
    "This functionality is written to provide a way to provide inputs to program with using UI (in a headless server version). This will be useful in case tool is run on a unix server connected via SSH.\n",
    "\n",
    "Function simply checks if _Config.xml_ file exists in the same location as tool.\n",
    "\n",
    "<span style=\"color:red\"><b>Need to add more checks for content of the file</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkxml():\n",
    "    \"\"\"Method to check if Config.xml file exists\n",
    "    \n",
    "    returns true if file exists\n",
    "    \"\"\"\n",
    "    return os.path.isfile(\"Config.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Data from Raw files in pandas dataframes** <a name=\"getdatamtd\"></a>\n",
    "\n",
    "In xml file with input file paths is not provided, program generates UI to prompt user to provide paths. In case user is running program in server mode (without UI), user is required to provide xml file only\n",
    "\n",
    "<span style=\"color:red\"><b>Please do note that current version only supports input data in excel format</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getdatafromsource():\n",
    "    \"\"\"Method to read Data from source files\n",
    "    \n",
    "    Method takes input on the file locations/databases either from xml file or from user input\n",
    "    reads all the data as pandas dataframes\n",
    "    stores input data as dictionary of dataframes\n",
    "    \"\"\"\n",
    "    #check if xml is not provided\n",
    "    if not checkxml():\n",
    "        app = Mappingpick(None) #generate UI to get mapping file. class automatically creates global dataframe for mapping\n",
    "        app.title(\"Mapping Type\") #provide name to UI window\n",
    "        app.mainloop() #run the UI app\n",
    "        app = Typepick(None) #generate UI to get input data type. class automatically creates global dataframe for input type\n",
    "        app.title(\"Input Type\")#provide name to UI window\n",
    "        app.mainloop()#run the UI app\n",
    "        if(InputType==3):\n",
    "            app = Datapick(None)#generate UI to get mapping file. class automatically creates global dict for data\n",
    "            app.title(\"InputData\")#provide name to UI window\n",
    "            app.mainloop()#run the UI app\n",
    "        app = Rulespick(None)\n",
    "        app.title(\"Rules\")#provide name to UI window\n",
    "        app.mainloop()#run the UI app\n",
    "    else: #if xml is provided\n",
    "        global mapping, entire_data, rulelist #Defining global variables\n",
    "        xm = BeautifulSoup(open(\"Config.xml\").read()) #read xml in formated way\n",
    "        mapping = pd.read_excel(xm.config.mappingfile.text) #read excel file for mapping\n",
    "        if xm.config.inputdata.type.text=='Excel': #if input type is excel\n",
    "            f=xm.config.inputdata.path.text #Input Excel File Path\n",
    "            wb = load_workbook(filename = f, read_only=True) #Load excel workbook\n",
    "            shtlst = wb.sheetnames #get worksheet names\n",
    "            entire_data = {} #create a dictionary \n",
    "            for i in range(len(shtlst)):\n",
    "                entire_data[shtlst[i]] = pd.read_excel(f, sheetname=shtlst[i]) #Add data from sheets in dictionary\n",
    "        rulelist = pd.read_excel(xm.config.ruledesc.text) #read rule descriptions as pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Rule List**\n",
    "\n",
    "Rule list is preprocessed based on the rule types, and the order in which the rules are provided. Please note that the ordering of rules is important to generate the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rulepreprosessing(rulelist, mapping):\n",
    "    \"\"\" Method to process the rule descriptions data\n",
    "    \n",
    "    Processes the rule descriptions dataframe to create a rank column, and actual dataframe and column name\n",
    "    dataframe originally contains table name and variable name in plain English, which is merged to Mapping Dataframe\n",
    "    A additional rank column is added based on rule type.This rank column decides what kind of visualization will be created.\n",
    "    returns a dataframe with merged and added columns \n",
    "    \"\"\"\n",
    "    \n",
    "    #merge the variable mappings - to get the actual variable name and dataframe name\n",
    "    Rules = rulelist.merge(mapping.drop(['Origin', 'Description'], axis=1), how='left', on=['Type of Data', 'Variable'])\n",
    "    #merge with mapping dataframe again to get the table and column name for dependent variables \n",
    "    #dependent variables are addtional variables for some integrity or completeness rules\n",
    "    Rules = Rules.merge(mapping.drop(['Origin', 'Description'], \\\n",
    "                        axis=1).rename(index=str, columns={\"Mapped Table\" : \"DepT\", \"Mapped Column\" : \"DepC\"}), \\\n",
    "                        how='left', left_on=['Type of Data', 'Depvar'], \\\n",
    "                        right_on=['Type of Data', 'Variable'])\n",
    "    #Create a rank based on dependent variable (highest priority), rule condition and rule sequence in the dataframe\n",
    "    Rules[\"rankf\"] = Rules.groupby(['Type of Data', 'Variable_x'])['DepC'].rank(method = 'dense', na_option ='bottom') *100 \\\n",
    "        +Rules.groupby(['Type of Data', 'Variable_x'])['C1'].rank(method = 'dense', na_option ='bottom') *10\\\n",
    "        + Rules.groupby(['Type of Data', 'Variable_x'])['Rule Id'].rank(method = 'dense', na_option ='bottom')\n",
    "    #create a new rank variable based on the rank created above\n",
    "    Rules[\"rank\"] = Rules.groupby(['Type of Data', 'Variable_x'])['rankf'].rank(method = 'dense', na_option ='bottom')\n",
    "    #drop redundant variable\n",
    "    Rules = Rules.drop(['rankf'], axis=1)\n",
    "    vlddf = Rules[(Rules['Dimension']=='Validity') & (Rules['C1']=='Range')]\n",
    "    return Rules, vlddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to check for missing value**\n",
    "\n",
    "Function checks if a value is NaN. works with both character and numeric. Similar function provided in numpy doesn't work with strings and unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isNan(x):\n",
    "    \"\"\"Method to check if a value is Nan\n",
    "    \n",
    "    Method has to be written as numpy isnan function throws error for character values\n",
    "    returns true if value is NaN\n",
    "    \"\"\"\n",
    "    return not (x==x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to check for Regex**\n",
    "\n",
    "Function checks if a regular expression occurs in a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def re_search(x, Criterion1):\n",
    "    \"\"\"Method to check if a string has a regex occurance\n",
    "    \n",
    "    returns true if regex occur within String\n",
    "    \"\"\"\n",
    "    if isNan(x):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return re.compile(Criterion1).search(x)!=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to create smooth buckets for a continuous variable** <a name=\"usefulfunc\"></a>\n",
    "\n",
    "A set of functions has been written in the code snippet below to create the smooth and clean buckets for the continuous variables. Instead of creating buckets at range/10, which may be a number in decimals, the method will create buckets ending in either on division of 10^x/4 or 10^x/2 or 10^x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMaxpositive(xinput):\n",
    "    \"\"\"Method to get the Max postive boundary value to be used to create buckets for a continuous variable\n",
    "    \n",
    "    Method takes the value to nearest value divisible by 10, 5 or 2.5 depending on the absolute value of input\n",
    "    \"\"\"\n",
    "    \n",
    "    xlog = math.log(xinput, 10) #get the log10 of value\n",
    "    \n",
    "    #take the value as 2.5 * floored log exponent. e.g. for 423, value will be 250\n",
    "    xlog1 = math.pow(10, math.floor(xlog)) *2.5 \n",
    "    \n",
    "    #take the value as 5 * floored log exponent. e.g. for 423, value will be 500\n",
    "    xlog2 = math.pow(10, math.floor(xlog)) *5\n",
    "    \n",
    "    #take the value as 10 * floored log exponent. e.g. for 423, value will be 1000\n",
    "    xlog3 = math.pow(10, math.floor(xlog)) *10\n",
    "    \n",
    "    #for values not between 1 and zero, cap the max value to min of the 3 values calculted above\n",
    "    if math.floor(xlog)!=0:\n",
    "        if xinput > xlog2:\n",
    "            return xlog3\n",
    "        elif xinput > xlog1:\n",
    "            return xlog2\n",
    "        else:\n",
    "            return xlog1\n",
    "    #for values between 1 and zero, cap the max value to min of the values calculated as 5 and 10 multipliers\n",
    "    else:\n",
    "        if xinput > xlog2:\n",
    "            return xlog3\n",
    "        else:\n",
    "            return xlog2\n",
    "\n",
    "def getminpositive(xinput):\n",
    "    \n",
    "    \"\"\"Method to get the Min postive boundary value to be used to create buckets for a continuous variable\n",
    "    \n",
    "    Method takes the value to nearest value divisible by 10, 5 or 2.5 depending on the absolute value of input\n",
    "    \"\"\"\n",
    "    \n",
    "    xlog = math.log(xinput, 10)#get the log10 of value\n",
    "    \n",
    "    #take the value as 2.5 * floored log exponent. e.g. for 423, value will be 250\n",
    "    xlog1 = math.pow(10, math.floor(xlog)) *2.5\n",
    "    \n",
    "    #take the value as 5 * floored log exponent. e.g. for 423, value will be 500\n",
    "    xlog2 = math.pow(10, math.floor(xlog)) *5\n",
    "    \n",
    "    #take the value as 10 * floored log exponent. e.g. for 423, value will be 1000\n",
    "    xlog3 = math.pow(10, math.floor(xlog)) *10\n",
    "    \n",
    "    #for values not between 1 and zero, cap the max value to max of the 3 values calculted above\n",
    "    if math.floor(xlog)!=0:\n",
    "        if xlog3 <= xinput:\n",
    "            return xlog3\n",
    "        elif xlog2 <= xinput:\n",
    "            return xlog2\n",
    "        else:\n",
    "            return xlog1\n",
    "    #for values between 1 and zero, cap the max value to max of the values calculated as 5 and 10 multipliers\n",
    "    else:\n",
    "        if xlog3 <= xinput:\n",
    "            return xlog3\n",
    "        else:\n",
    "            return xlog2\n",
    "\n",
    "def getMaxSmooth(xinput):\n",
    "    \"\"\"\n",
    "    Method to get Maximum smoothened boundary value for givne input\n",
    "    \n",
    "    Works with both positive and negative values\n",
    "    \"\"\"\n",
    "    if xinput>0: # for postive input\n",
    "        return getMaxpositive(xinput) #return positive max value\n",
    "    elif xinput<0: #for negative value\n",
    "        return -getminpositive(math.fabs(xinput)) #get negative of min of absolute value \n",
    "    else: #return zero for 0 values\n",
    "        return 0\n",
    "    \n",
    "def getMinSmooth(xinput):\n",
    "    \"\"\"\n",
    "    Method to get minimum smoothened boundary value for givne input\n",
    "    \n",
    "    Works with both positive and negative values\n",
    "    \"\"\"\n",
    "    \n",
    "    if xinput>0: # for postive input\n",
    "        return getminpositive(xinput)#return positive min value\n",
    "    elif xinput<0:#for negative value\n",
    "        return -getMaxpositive(math.fabs(xinput)) #get negative of max of absolute value\n",
    "    else:\n",
    "        return 0 #return zero for 0 values\n",
    "\n",
    "def smoothenmin(xinput, smoothvar):\n",
    "    \"\"\"Method to smooth on the given value on lower side by smoothening buckets\n",
    "    \n",
    "    4.3 for a 2 smoothening, will return 4 and 4.3 for a 3 smootheing will return 3\n",
    "    returns the max value less than xinput but divisible by smoothvar\n",
    "    \"\"\"\n",
    "    return math.floor(xinput/smoothvar) * smoothvar\n",
    "\n",
    "def smoothenmax(xinput, smoothvar):\n",
    "    \"\"\"Method to smooth the given value on upper side by smoothening buckets\n",
    "    \n",
    "    4.3 for a 2 smoothening, will return 6 and 4.3 for a 2.5 smootheing will return 5 \n",
    "    returns the min value greater than xinput but divisible by smoothvar\n",
    "    \"\"\"\n",
    "    \n",
    "    return (math.floor(xinput/smoothvar) +1)* smoothvar\n",
    "\n",
    "def getbuckets(inparr):\n",
    "    \"\"\"Method to create proper and smooth buckets for a continuous variable\n",
    "    \n",
    "    Method is used to create the buckets which is later used to create histogram like visualiztions\n",
    "    \"\"\"\n",
    "    \n",
    "    # the buckets the created between the smoothened boundaries\n",
    "    diffx =  (getMaxSmooth(inparr.max()) - getMinSmooth(inparr.min()))/10 #get the range of values\n",
    "    \n",
    "    #difference is smoothened again to maximum\n",
    "    diffx = getMaxSmooth(diffx)\n",
    "    \n",
    "    #minimum is determined based on the value of diffence and actual minimum\n",
    "    if(getMinSmooth(inparr.min()) < 1) and (diffx >=10):\n",
    "        xmin = math.floor(getMinSmooth(inparr.min()))\n",
    "    else:\n",
    "        xmin = getMinSmooth(inparr.min())\n",
    "    \n",
    "    #create the the linear buckets by values  \n",
    "    pctinp = np.linspace(xmin, xmin + diffx*10, 11)\n",
    "    \n",
    "    #assign the buckets to series\n",
    "    return pd.cut(inparr, pctinp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to create rule descriptions to show on hover**\n",
    "\n",
    "Create a description based on the rule description provided. Function to be applied over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prrulesDesc(x):\n",
    "    if not isNan(x['Depvar']):\n",
    "        t1 = x['Criterion']\n",
    "        if t1 =='this':\n",
    "            t1 = x['Variable_x']\n",
    "        pr1 = x['Depvar'] + \" \" + x['Condition'] + \" \"+ str(t1)\n",
    "    else:\n",
    "        pr1 = \"\"\n",
    "    if not isNan(x['C1']):\n",
    "        if pr1==\"\":\n",
    "            pr1 = pr1 + x['C1'] + \" : \" + x['Criterion1']\n",
    "        else:\n",
    "            pr1 = pr1 + \" | \" + x['C1'] + \" : \" + x['Criterion1']\n",
    "    return pr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to classify CPT codes in Logical Buckets**\n",
    "\n",
    "CPT codes can be grouped into logical buckets as provided [here](https://en.wikipedia.org/wiki/Current_Procedural_Terminology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processCPT(x, criterion):\n",
    "    t = re_search(x, criterion)\n",
    "    if not np.isnan(t) and t:\n",
    "        x = int(x)\n",
    "        if x >= 99210 and x<= 99499:\n",
    "            z = 'Evaluation and Management'\n",
    "        elif (x >= 100 and x<= 1999) or (x>=99100 and x<= 99150):\n",
    "            z = 'Anesthesia'\n",
    "        elif x>=10000 and x<=69990:\n",
    "            z = 'Surgery'\n",
    "        elif x>=70000 and x<= 79999:\n",
    "            z = 'Radiology'\n",
    "        elif x>=80000 and x<= 89398:\n",
    "            z = 'Pathology and Laboratory'\n",
    "        elif (x>=90281 and x<= 99099) or (x>= 99151 and x<= 99199) or (x>=99500 and x<=99607):\n",
    "            z = 'Medicine'\n",
    "        else:\n",
    "            z = 'Invalid'\n",
    "    elif not t:\n",
    "        z = 'Invalid'\n",
    "    else:\n",
    "        z = np.NaN\n",
    "    return z        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to classify ICD10 codes in logical buckets**\n",
    "\n",
    "ICD10 codes can be grouped into logical buckets as provided [here](http://www.icd10data.com/ICD10CM/Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processICD10(x, criterion):\n",
    "    t = re_search(x, criterion)\n",
    "    if not np.isnan(t) and t:\n",
    "        x = x[:3]\n",
    "        if x <= 'B99':\n",
    "            z = 'infectious and parasitic'\n",
    "        elif x<='D49':\n",
    "            z = 'Neoplasms'\n",
    "        elif x<='D89':\n",
    "            z = 'blood & immunity'\n",
    "        elif x<='E89':\n",
    "            z = 'Endocrine, nutritional and metabolic'\n",
    "        elif x<='F99':\n",
    "            z = 'Mental, Behavioral and Neurodevelopmental'\n",
    "        elif x<= 'G99':\n",
    "            z = 'nervous system'\n",
    "        elif x<= 'H59':\n",
    "            z = 'eye and adnexa'\n",
    "        elif x<= 'H99':\n",
    "            z = 'ear and mastoid process'\n",
    "        elif x<= 'I99':\n",
    "            z = 'circulatory system'\n",
    "        elif x<= 'J99':\n",
    "            z = 'respiratory system'\n",
    "        elif x<= 'K99':\n",
    "            z = 'digestive system'\n",
    "        elif x<= 'L99':\n",
    "            z = 'skin and subcutaneous tissue'\n",
    "        elif x<= 'M99':\n",
    "            z = 'musculoskeletal system and connective tissue'\n",
    "        elif x<= 'N99':\n",
    "            z = 'genitourinary system'\n",
    "        elif x<='O99':\n",
    "            z = 'Pregnancy, childbirth and the puerperium'\n",
    "        elif x<= 'P99':\n",
    "            z= 'conditions originating in the perinatal period'\n",
    "        elif x<= 'Q99':\n",
    "            z = 'Congenital malformations'\n",
    "        elif x<= 'R99':\n",
    "            z = 'abnormal clinical and laboratory findings'\n",
    "        elif x<= 'T99':\n",
    "            z = 'Injury and external causes'\n",
    "        elif x<= 'Y99':\n",
    "            z= 'External causes of morbidity'\n",
    "        elif x<= 'Z99':\n",
    "            z = 'Factors influencing health status'\n",
    "        else:\n",
    "            z = 'Invalid'\n",
    "    elif not t:\n",
    "        z = 'Invalid'\n",
    "    else:\n",
    "        z = np.NaN\n",
    "    return z        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting data from source <a name=\"getdata\"></a>\n",
    "\n",
    "Running the [method](#getdatamtd) to get data from source files/databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getdatafromsource()\n",
    "Rules, vlddf = rulepreprosessing(rulelist, mapping)\n",
    "Rules['DescriptionToshow'] = Rules.apply(prrulesDesc, axis=1)\n",
    "RulesDescdf = Rules.astype(str).groupby(['Type of Data', 'Variable_x', 'Dimension'])['DescriptionToshow']\\\n",
    ".agg(lambda col: ' ; '.join(col)).reset_index()\n",
    "RulesDescdf.columns = ['Type of Data', 'Variable', 'Dimension', 'DescriptionToshow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalution of a Rule<a name=\"Ruleeval\"></a>\n",
    "\n",
    "The method below evaluate a rule, as described in rule description file. Method first checks is there is any condition which needs to be evaluted using a dependent variable, it first evaluates that condition, and creates a dataframe which will be used for plotting.\n",
    "\n",
    "In the second step, method applies any validity/accuracy rule to on the data, and finally calculates the score for the rule depending on to which dimension rule belongs. \n",
    "\n",
    "Method outputs a dataframe with data which will be used for plotting, and a list with rule score.\n",
    "\n",
    "see [functions](#usefulfunc) being used in method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to process a Rule and determine the Data Accuracy Based on Single Variable\n",
    "def processvar(i):\n",
    "    \"\"\" Method to check the data for a rule\n",
    "    \n",
    "    Method pulls the data from the source data, processes the rules, and then exports the summary in a dataframe\n",
    "    Method also summarizes data, just from the charting perspective\n",
    "    \"\"\"\n",
    "    typedata = Rules['Type of Data'][i] #Data Type\n",
    "    varx = Rules['Variable_x'][i] # Variable to be Used\n",
    "    rid = Rules['Rule Id'][i] # Rule Id\n",
    "    wt = Rules['Weight'][i] # Weight of the Rule\n",
    "    dim = Rules['Dimension'][i] #getting the dimension of Rule\n",
    "    op=None #Initializing Output Data for charts\n",
    "    thisline=None #Initializing Output Summary data\n",
    "    #Checking if Data Table Exists in Mapping\n",
    "    if not isNan(Rules['Mapped Table'][i]):\n",
    "        x = entire_data[Rules['Mapped Table'][i]][Rules['Mapped Column'][i]] # Getting Actual Column Values\n",
    "        if varx.upper().find('DATE')>-1:\n",
    "            x = pd.to_datetime(x)\n",
    "        #checking if not all values are missing\n",
    "        if x.isnull().all()==False:\n",
    "            dimsupp = Rules['DepT'][i] #getting the Table for dependent variable used in the rule\n",
    "            #if rank of the rule is 1, need to process data for the charting\n",
    "            if Rules['rank'][i]==1:\n",
    "                #check if variable is a number\n",
    "                if x.dtypes in [np.number]:\n",
    "                    col1_br = getbuckets(x) #create smooth buckets for column\n",
    "                # check if variable is a Date\n",
    "                elif x.dtypes in ['<M8[ns]', '>M8[ns]', np.datetime64]:\n",
    "                    col1_br = x.apply(lambda x: x.year*100+x.month) #create buckets as yyyymm\n",
    "                    col1_br = col1_br.fillna(-9999999).astype(np.int64) #fill missing values and convert to numeric\n",
    "                    if len(col1_br.value_counts())>36:\n",
    "                        col1_br = x.apply(lambda x: x.year) #create buckets as yyyy\n",
    "                        col1_br = col1_br.fillna(-9999999).astype(np.int64) #fill missing values and convert to numeric\n",
    "                        if len(col1_br.value_counts())>30:\n",
    "                            #create buckets as yyyy - in 5 years range\n",
    "                            col1_br = col1_br.apply(lambda x: x if x==-9999999 else int(x/5)*5) \n",
    "                            if len(col1_br.value_counts())>30:\n",
    "                                #create buckets as yyyy - in 10 years range\n",
    "                                col1_br = col1_br.apply(lambda x: x if x==-9999999 else int(x/10)*10) \n",
    "                            \n",
    "                else:\n",
    "                    col1_br = x.copy() #for character, no bucketing is done\n",
    "                    col_count = pd.Series(x.value_counts().index) #getting unique value frequnecy of character columns\n",
    "                    # if character column has more than 8 unique values\n",
    "                    if len(col_count)>=8:\n",
    "                        col_count_lst = col_count[:7].tolist() # Take Top 8 values by count\n",
    "                        col1_br[~col1_br.isin(col_count_lst)] = 'Other' # Club all further values as Others \n",
    "                    col1_br_str = col1_br.astype(str) #Convert values to String\n",
    "                    #check if more than 90% values are \"Other\"\n",
    "                    #90% number can be changed based on business usecase\n",
    "                    if len(col1_br[col1_br_str=='Other'])*1.0/len(col1_br)>0.9:\n",
    "                        c1 = Rules['C1'][i] #Condition to be applied variable\n",
    "                        Criterion1 = Rules['Criterion1'][i] #Criteria Value to be Applied\n",
    "                        #if there is criteria provided\n",
    "                        if not isNan(c1):\n",
    "                            #if criteria is Regex\n",
    "                            if Criterion1.startswith(\"Regex::\"):\n",
    "                                Criterion1 = Criterion1.replace(\"Regex::\", \"\") #create regex from String\n",
    "                                if varx.upper().find('ICD')>-1:\n",
    "                                    #create column validity by regex\n",
    "                                    col1_br = x.apply(processICD10, args=(Criterion1,)).astype(str)\n",
    "                                elif varx.upper().find('TREATMENT CODES')>-1:\n",
    "                                    col1_br = x.apply(processCPT, args=(Criterion1,)).astype(str)\n",
    "                                else:\n",
    "                                    col1_br = x.apply(re_search, args=(Criterion1,)).astype(str) #create column validity by regex\n",
    "                            else:\n",
    "                                col1_br = x.apply(isNan) #create column values by cheking if values are mssing (for charting)\n",
    "                        else:\n",
    "                            col1_br = x.value_counts().reset_index(drop=True)#simply use values for column charts\n",
    "\n",
    "                col1_br.name = Rules['Mapped Column'][i] #Rename the column                 \n",
    "            #check if dependent column exist\n",
    "            if not isNan(dimsupp): \n",
    "                y = entire_data[Rules['DepT'][i]][Rules['DepC'][i]] #Take the dependent variable in a column\n",
    "                cond1 = Rules['Condition'][i] #Condition on Dependent Variable\n",
    "                crit1 = Rules['Criterion'][i] #Criterion for Dependent Variable\n",
    "                # if condition for dependent variable is to check values within a list or range\n",
    "                if cond1=='in':\n",
    "                    #processing the values for range or list\n",
    "                    crit1 = crit1.replace(\"(\", \"\").replace(\")\", \"\").replace(\"'\", \"\").split(\",\")\n",
    "                    if np.issubdtype(y.dtype, np.number):\n",
    "                        # Treating numbers in right format\n",
    "                        crit1 = [float(internal1) for internal1 in crit1]\n",
    "                    x2 = x[y.isin(crit1)] #checking if condition is met\n",
    "                    m = y.copy() #creating a copy of dependent variable for charting\n",
    "                elif cond1==\">=\":\n",
    "                    # if condtion is on inequality\n",
    "                    if(crit1==\"this\"):\n",
    "                        #if criteria is to compare with current variable being evaluated\n",
    "                        x2 = x[y.astype(x.dtype)>=x] #creating filtered data\n",
    "                        m1 = y.astype(x.dtype)>=x #creating a boolean series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" >= \" +  Rules['Variable_x'][i]\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" < \" +  Rules['Variable_x'][i]\n",
    "                    else:\n",
    "                        # if criteria is to compare with a defined scaler\n",
    "                        x2 = x[y>=float(crit1)] #creating filtered data\n",
    "                        m1 = y>=float(crit1) #creating booelan series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" >= \" + str(crit1)\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" < \" + str(crit1)\n",
    "                elif cond1==\"<=\":\n",
    "                    # if condtion is on inequality\n",
    "                    if(crit1==\"this\"):\n",
    "                        #if criteria is to compare with current variable being evaluated\n",
    "                        x2 = x[y.astype(x.dtype)<=x]#creating filtered data\n",
    "                        m1 = y.astype(x.dtype)<=x #creating a boolean series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" <= \" +  Rules['Variable_x'][i]\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" > \" +  Rules['Variable_x'][i]\n",
    "                    else:\n",
    "                        # if criteria is to compare with a defined scaler\n",
    "                        x2 = x[y<=float(crit1)] #creating filtered data\n",
    "                        m1 = y<=float(crit1) #creating booelan series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" <= \" + str(crit1)\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" > \" + str(crit1)\n",
    "                elif cond1==\">\":\n",
    "                    # if condtion is on strict inequality\n",
    "                    if(crit1==\"this\"):\n",
    "                        #if criteria is to compare with current variable being evaluated\n",
    "                        x2 = x[y.astype(x.dtype)>x] #creating filtered data\n",
    "                        m1 = y.astype(x.dtype)>x #creating a boolean series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" > \" +  Rules['Variable_x'][i]\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" <= \" +  Rules['Variable_x'][i]\n",
    "                    else:\n",
    "                        # if criteria is to compare with a defined scaler\n",
    "                        x2 = x[y>float(crit1)] #creating filtered data\n",
    "                        m1 = y>float(crit1) #creating booelan series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" > \" + str(crit1)\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" <= \" + str(crit1)\n",
    "                elif cond1==\"<\":\n",
    "                    # if condtion is on strict inequality\n",
    "                    if(crit1==\"this\"):\n",
    "                        #if criteria is to compare with current variable being evaluated\n",
    "                        x2 = x[y.astype(x.dtype)<x] #creating filtered data\n",
    "                        m1 = y.astype(x.dtype)<x #creating a boolean series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" < \" +  Rules['Variable_x'][i]\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" >= \" +  Rules['Variable_x'][i]\n",
    "                    else:\n",
    "                        # if criteria is to compare with a defined scaler\n",
    "                        x2 = x[y<=float(crit1)] #creating filtered data\n",
    "                        m1 = y<=float(crit1) #creating booelan series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" < \" + str(crit1)\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" >= \" + str(crit1)\n",
    "                else:\n",
    "                    # if condtion is on strict equality\n",
    "                    if(crit1==\"this\"):\n",
    "                        #if criteria is to compare with current variable being evaluated\n",
    "                        x2 = x[y.astype(x.dtype)==x] #creating filtered data\n",
    "                        m1 = y.astype(x.dtype)==x #creating a boolean series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" == \" +  Rules['Variable_x'][i]\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" != \" +  Rules['Variable_x'][i]\n",
    "                    else:\n",
    "                        # if criteria is to compare with a defined scaler\n",
    "                        x2 = x[y==float(crit1)] #creating filtered data\n",
    "                        m1 = y==float(crit1) #creating booelan series\n",
    "                        m = m1.copy() #copying boolean series\n",
    "                        #creating text labels for charting\n",
    "                        m[m1] = Rules['Depvar'][i] + \" == \" + str(crit1)\n",
    "                        m[~m1] = Rules['Depvar'][i] + \" != \" + str(crit1)\n",
    "                if Rules['rank'][i]==1:\n",
    "                    #creating a dataframe for charing purposes\n",
    "                    col2_br = m #second dimension for charting\n",
    "                    col2_br.name = Rules['DepC'][i] #renaming second dimension\n",
    "                    tmpdf = pd.concat([col1_br, col2_br], axis=1).reset_index()#creating dataframe of data check results\n",
    "                    op = tmpdf.groupby([Rules['Mapped Column'][i], Rules['DepC'][i]]).count().reset_index() #summarizing by count\n",
    "                    op.columns = ['ChartValue1', 'ChartValue2', 'ChartValue3'] #renaming for charting\n",
    "                    op['ChartCol1'] = Rules['Mapped Column'][i] #Adding variable name in a column\n",
    "                    op['ChartCol2'] = Rules['DepC'][i]#adding dependent variable name in a column\n",
    "            else:\n",
    "                #in case no dependent variable has been defined\n",
    "                x2 = x.copy() #copy variable without applying any filter. this series to be used in further calculation\n",
    "                #create the charting data for rank1 rules (please note that only rank 1 rules are plotted)\n",
    "                #if there is no dependent dimension, leave the dimension and column blank in charting data\n",
    "                if Rules['rank'][i]==1:\n",
    "                    tmpdf = pd.DataFrame(col1_br).reset_index() # create dataframe with just 1 column\n",
    "                    op = tmpdf.groupby([Rules['Mapped Column'][i]]).count().reset_index() #summarize for counts\n",
    "                    #rename columns and add empty columns\n",
    "                    op.columns = ['ChartValue1', 'ChartValue3']\n",
    "                    op['ChartValue2'] = None\n",
    "                    op['ChartCol1'] = Rules['Mapped Column'][i]\n",
    "                    op['ChartCol2'] =None        \n",
    "            c1 = Rules['C1'][i] #Condtion on this variable (generaly validity or accuracy rules)\n",
    "            Criterion1 = Rules['Criterion1'][i] #criteria for the condition\n",
    "            if not isNan(c1):\n",
    "                # if a criteria has been provided\n",
    "                if c1==\"Values\":\n",
    "                    # condition is to check the variable values within a list of values\n",
    "                    # clean up the list of values\n",
    "                    # in file list is provided as '(list of values separated by commas)'\n",
    "                    # parantheses are stripped\n",
    "                    # text is converted into list of values\n",
    "                    Criterion1 = Criterion1.replace(\"(\", \"\").replace(\")\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "                    # remove any leading and triling spaces\n",
    "                    Criterion1 = [tin1.strip() for tin1 in Criterion1]\n",
    "                    # if list is of numbers, convert strings to numbers\n",
    "                    if np.issubdtype(x2.dtype, np.number):\n",
    "                        Criterion1 = [float(t) for t in Criterion1]\n",
    "                    # filter data if either values are in list or missing\n",
    "                    x1 = x2[(x2.isin(Criterion1)) | (x2.apply(isNan))]\n",
    "                elif c1==\"Range\":\n",
    "                    # condition is to check if value is within a range\n",
    "                    # only closed ended criterion can be checked for now\n",
    "                    # assumed that range is provided in '[<lower Bound>, <Upper Bound> format]\n",
    "                    # works only for numbers and Dates\n",
    "                    # dates should be provided in mm/dd/yyyy format\n",
    "                    # removing brackets and creating list of numbers/dates in string format\n",
    "                    Criterion1 = Criterion1.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\",\")\n",
    "                    # if column being assessed is a number, convert string to numbers\n",
    "                    if np.issubdtype(x2.dtype, np.number):\n",
    "                        Criterion1 = [float(t) for t in Criterion1]\n",
    "                    if x2.dtypes in ['<M8[ns]', '>M8[ns]', np.datetime64, np.dtype('O')]:\n",
    "                        # if column being assessed is date, reading values as date\n",
    "                        Criterion1 = [datetime.strptime(t.replace(\" \", \"\"), '%m/%d/%Y') for t in Criterion1]\n",
    "                    # Checking if the values meet the criteria or values are missing\n",
    "                    x1 = x2[(x2 >= Criterion1[0]) | (x2.apply(isNan))]\n",
    "                    x1 = x1[(x1 <= Criterion1[1]) | (x1.apply(isNan))]\n",
    "                elif c1==\"Format\":\n",
    "                    # condtion if to check the format of column\n",
    "                    if Criterion1==\"Number\":\n",
    "                        # need to check if values are numbers\n",
    "                        # Relying on intelligence of pandas package\n",
    "                        # if values are read as numbers, then return entire series, else return empty series\n",
    "                        if np.issubdtype(x.dtype, np.number):\n",
    "                            x1=x2\n",
    "                        else:\n",
    "                            x1=pd.Series()\n",
    "                    elif Criterion1.startswith(\"Regex::\"):\n",
    "                        # need to check if string follows regex\n",
    "                        # create regex from given value\n",
    "                        # Assumed that regex are provided as \"Regex::<regex>\"\n",
    "                        Criterion1 = Criterion1.replace(\"Regex::\", \"\")\n",
    "                        # check if values follow regex format or are missing\n",
    "                        x1 = x2[(x2.apply(re_search, args=(Criterion1,))) | (x2.apply(isNan))]\n",
    "                    else:\n",
    "                        x1=x2\n",
    "                else:\n",
    "                    # remove empty series for any other type of condition\n",
    "                    x1=pd.Series()\n",
    "            # in case no condition is to be applied, remove the original series\n",
    "            else:\n",
    "                x1=x2.copy()\n",
    "            if Rules['rank'][i]==1:\n",
    "                # if rank is 1 generate the output data for charting\n",
    "                #converting values to String (Categories for column charts)\n",
    "                op['ChartValue1'] = op['ChartValue1'].astype(str)\n",
    "                op['ChartValue2'] = op['ChartValue2'].astype(str)\n",
    "                #Renaming Missing Values\n",
    "                op['ChartValue1'][op['ChartValue1']=='-9999999'] = 'Missing'\n",
    "                op['ChartValue2'][op['ChartValue2']=='-9999999'] = 'Missing'\n",
    "                op['ChartValue1'][op['ChartValue1']=='nan'] = 'Missing'\n",
    "                op['ChartValue1'][op['ChartValue1']=='True'] = 'Valid'\n",
    "                op['ChartValue1'][op['ChartValue1']=='False'] = 'Invalid'\n",
    "                op['Type of Data'] = typedata\n",
    "                op['Variable'] = varx\n",
    "                op['Chart Number'] = i\n",
    "                op = op[op['ChartValue3']>0]\n",
    "        else:\n",
    "            # for all values as missing\n",
    "            x=pd.Series()\n",
    "    else:\n",
    "        # no such variable exists\n",
    "        x=pd.Series()\n",
    "    dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])\n",
    "    if not isNan(dim):\n",
    "        if len(x)==0:\n",
    "            # for missing values, score is zero\n",
    "            thisline = [typedata, varx, rid, wt, dim, 0]\n",
    "            dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])                    \n",
    "        else:\n",
    "            x2\n",
    "            if dim==\"Completeness\":\n",
    "                #calculate completeness score\n",
    "                if len(x2)>0:\n",
    "                    score = x2.count()*1.0/len(x2)\n",
    "                    xnew = x2[x2.apply(isNan)]\n",
    "                    dfout1 = pd.DataFrame(xnew)\n",
    "                    dfout1.columns = ['Value']\n",
    "                    dfout1['origIndex'] = dfout1.index\n",
    "                    dfout1['Type of Data'] = typedata\n",
    "                    dfout1['Variable'] = varx\n",
    "                    dfout1['Dimension'] = dim\n",
    "                    dfout1['RuleID'] = rid\n",
    "                else:\n",
    "                    score = 0\n",
    "                    dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])                    \n",
    "                thisline = [typedata, varx, rid, wt, dim, score]\n",
    "            if dim==\"Validity\":\n",
    "                #Calculate Validity score\n",
    "                if len(x2) > 0 :\n",
    "                    score = x1.count()*1.0/x2.count()\n",
    "                    dfout1 = pd.DataFrame(x2[pd.Index.difference(x2.index, x1.index)])\n",
    "                    dfout1.columns = ['Value']\n",
    "                    dfout1['origIndex'] = dfout1.index\n",
    "                    dfout1['Type of Data'] = typedata\n",
    "                    dfout1['Variable'] = varx\n",
    "                    dfout1['Dimension'] = dim\n",
    "                    dfout1['RuleID'] = rid\n",
    "                else:\n",
    "                    score = 0\n",
    "                    dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])\n",
    "                thisline = [typedata, varx, rid, wt, dim, score]\n",
    "            if dim==\"Integrity\":\n",
    "                #calculate integrity score\n",
    "                if len(x) > 0:\n",
    "                    score = x2.count()*1.0/x.count()\n",
    "                    dfout1 = pd.DataFrame(x[pd.Index.difference(x.index, x2.index)])\n",
    "                    dfout1.columns = ['Value']\n",
    "                    dfout1['origIndex'] = dfout1.index\n",
    "                    dfout1['Type of Data'] = typedata\n",
    "                    dfout1['Variable'] = varx\n",
    "                    dfout1['Dimension'] = dim\n",
    "                    dfout1['RuleID'] = rid\n",
    "                else:\n",
    "                    score=0\n",
    "                    dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])\n",
    "                thisline = [typedata, varx, rid, wt, dim, score]\n",
    "            if dim==\"Uniqueness\":\n",
    "                #calculate uniqueness score\n",
    "                if len(x2)>0:\n",
    "                    score = x2.nunique()*1.0/x2.count()\n",
    "                    dfout1 = pd.DataFrame(x2[x2.duplicated(keep=False)])\n",
    "                    dfout1.columns = ['Value']\n",
    "                    dfout1['origIndex'] = dfout1.index\n",
    "                    dfout1['Type of Data'] = typedata\n",
    "                    dfout1['Variable'] = varx\n",
    "                    dfout1['Dimension'] = dim\n",
    "                    dfout1['RuleID'] = rid\n",
    "                else:\n",
    "                    score=0\n",
    "                    dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])\n",
    "                thisline = [typedata, varx, rid, wt, dim, score]\n",
    "            if dim==\"Accuracy\":\n",
    "                #calculate Accuracy Score\n",
    "                if len(x)>0:\n",
    "                    score = x1.count()*1.0/x2.count()\n",
    "                    dfout1 = pd.DataFrame(x2[pd.Index.difference(x2.index, x1.index)])\n",
    "                    dfout1.columns = ['Value']\n",
    "                    dfout1['origIndex'] = dfout1.index\n",
    "                    dfout1['Type of Data'] = typedata\n",
    "                    dfout1['Variable'] = varx\n",
    "                    dfout1['Dimension'] = dim\n",
    "                    dfout1['RuleID'] = rid\n",
    "                else:\n",
    "                    score=0\n",
    "                    dfout1 = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])\n",
    "                thisline = [typedata, varx, rid, wt, dim, score]\n",
    "    return op, thisline, dfout1 #return charting dataset and score for the rule in list format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rule Calculation<a name=\"rulecalc\"></a>\n",
    "\n",
    "[Method](#Ruleeval) is run to evaluate all the ruls as described in input files on the input data. The entire chart data and rules evaluation data are saved as two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None #option to suppress slice warning on dataframe\n",
    "#Defining an empty dataframe for Rule Scoring Output\n",
    "outDF = pd.DataFrame(columns = ['Type of Data', 'Variable', 'Rule Id', 'Weight', 'Dimension', 'Score'])\n",
    "#Defining an empty dataframe for storing chart Data\n",
    "chartDF = pd.DataFrame(columns = ['Type of Data', 'Variable',\n",
    "                 'Chart Number', 'ChartValue1', 'ChartValue2', 'ChartValue3', 'ChartCol1', 'ChartCol2'])\n",
    "errorDF = pd.DataFrame(columns = ['Value', 'origIndex', 'Type of Data', 'Variable', 'Dimension', 'RuleID'])\n",
    "#Running rule evaluation for all the rules\n",
    "for i in range(len(Rules)):\n",
    "    op, thisline, dfout1 = processvar(i) #using method to process the rule\n",
    "    if op is not None:\n",
    "        # if there is a chart output, append it to chart DataFrame\n",
    "        chartDF = pd.concat([chartDF, op], axis=0)\n",
    "    if thisline is not None:\n",
    "        # if there is an output for rule score, append it to Rule Score DataFrame\n",
    "        outDF.loc[outDF.shape[0]] =  thisline\n",
    "    if dfout1 is not None:\n",
    "        errorDF = pd.concat([errorDF, dfout1], axis=0)\n",
    "errorDF.fillna(value=-99999, inplace=True)\n",
    "errorDF = errorDF[['Type of Data', 'Variable', 'Dimension', 'RuleID', 'origIndex', 'Value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Summarization<a name=\"rulesumm\"></a>\n",
    "\n",
    "Summarizing the Rules score dataset in various ways for display in visualization.\n",
    "\n",
    "In this section, input data is also deleted from program memory, to make program run faster\n",
    "\n",
    "<span style=\"color:blue\"><b>Please change this portion of code to change the summarization logic based on weight</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxcharts = chartDF.groupby(['ChartCol1', 'ChartValue2'])['Variable'].count().reset_index().\\\n",
    "groupby(['ChartCol1'])['ChartValue2'].count().\\\n",
    "reset_index()['ChartValue2'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = {'Score': lambda x: np.average(x, weights=outDF.loc[x.index, \"Weight\"])} #weighted mean aggregation function\n",
    "# summarizing score by variable - weighted by individual rule level weights\n",
    "scoredf = outDF.groupby(['Type of Data', 'Variable', 'Dimension']).agg(f).reset_index()\n",
    "scoredf = scoredf.groupby(['Type of Data', 'Variable'])['Score'].prod().reset_index()\n",
    "#Summarizing by variable and dimensions - weighted by individual rule level weights\n",
    "scoredf['Dimension'] = 'Overall'\n",
    "scoredfDimension = outDF.groupby(['Type of Data', 'Variable', 'Dimension']).agg(f).reset_index()\n",
    "scoredf = pd.concat([scoredfDimension, scoredf], axis=0)\n",
    "#Summarizing by dimensions across all variables - weighted by individual rule level weights \n",
    "scoreOvDimension = scoredfDimension.groupby(['Dimension'])['Score'].mean().reset_index()\n",
    "#Summarizing at overall level - weighted by individual rule level weights\n",
    "scoref = scoreOvDimension.groupby(lambda idx: 0)['Score'].prod().reset_index()\n",
    "#getting overall score number\n",
    "oscore = scoref['Score'][0]\n",
    "scoredfDimension = pd.merge(scoredfDimension, RulesDescdf, on = ['Type of Data', 'Variable', 'Dimension'], how='left')\n",
    "#deleting input data from memory\n",
    "#entire_data=None\n",
    "#removing reference\n",
    "#del entire_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Tool <a name=\"VizTool\"></a>\n",
    "\n",
    "The code in this part is majorly concerned with development and rendering of visualization. To create interactive visualizations, [bokeh](#https://bokeh.pydata.org/en/latest/) is used.\n",
    "\n",
    ">_Bokeh is a Python interactive visualization library that targets modern web browsers for presentation\n",
    ">Its goal is to provide elegant, concise construction of novel graphics in the style of D3.js, and \n",
    ">to extend this capability with high-performance interactivity over very large or streaming datasets_\n",
    "\n",
    "To make visulizations interact with user, and reflect requested changes, bokeh widgets are used and javascript callback functions are used to interact with Python datasets \n",
    "\n",
    "The tool has two tabs. First tab is a summary tab in which the overall score and dimesion-wise score are provided. Dimension-wise scores are provided in a table, which is clickable. Once a dimension is clicked, on the bottom half of the screen will have the description of selected dimension\n",
    "\n",
    "On the second tab, screen is divided in two columns, the left half is a table, in which overall score and dimension wise score for each dimension is provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BarChart Method<a name=\"Barchart\"></a>\n",
    "\n",
    "A method to draw bar chart using basic glyphs. high level barchart is not being used as that is always resizable and creates issues in final visualization. Also, high level Barchart doesn't provide enough flexibility, and hence creating charts from basic glyphs\n",
    "\n",
    "Given a figure object and and Dataframe, if there are more than one class, iteratively plot all the classes on top of each other (stacked). Missing values are put as a Caveat - call out instead of plotting as a separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Method to create a Bar Chart\n",
    "def makebarchart(p, df): \n",
    "    \"\"\"Method to create a bar chart\n",
    "    \n",
    "    Given a figure object and and Dataframe, if there are more than one class, \n",
    "    iteratively plot all the classes on top of each other (stacked). \n",
    "    Missing values are put as a Caveat - call out instead of plotting as a separate column\n",
    "    \"\"\"\n",
    "    \n",
    "    miss = df[df['ChartValue1']=='Missing']#Take out missing values in a separate dataframe\n",
    "    df = df[~(df['ChartValue1']=='Missing')]#Keep non-missing categories data\n",
    "    cntmiss = miss['ChartValue3'].sum() #get nuber of missing values for callout\n",
    "    totcnt = df['ChartValue2'].nunique() #number of unique stacking categories to be plotted\n",
    "    p.y_range.start=0 # set the chart y-axis to be started at zero\n",
    "    r = []\n",
    "    legends = []\n",
    "    if totcnt>1:\n",
    "        # check if there are more than one stacking category, i.e. stacking needs to be done on chart\n",
    "        #Summarize data by the category and stacking values\n",
    "        #unstack the value to create wide data\n",
    "        #using groupby and unstack as order of the stacking and group needs to be retained\n",
    "        #pivot does not reatin the order \n",
    "        df = df.groupby(['Variable', 'ChartCol2', 'ChartValue1', 'ChartValue2'], \\\n",
    "                        sort=False)['ChartValue3'].sum().unstack('ChartValue2').reset_index().fillna(0)\n",
    "        #getting the names of stacking from column Names\n",
    "        legs1 = df.columns[3:]\n",
    "        #setting the X-axis range to Category Values in datafarme\n",
    "        p.x_range.factors = df['ChartValue1'].tolist()\n",
    "        #creating x-values list for plotting\n",
    "        xval = list(range(1, len(df)+1))\n",
    "        for i in range(maxcharts):\n",
    "            if i<len(legs1):\n",
    "                #iteratively add all the stack bars on chart\n",
    "                if i>=1:\n",
    "                    #for any addtional stack (not the first stack), create the top value by adding the last value\n",
    "                    df[legs1[i]] = df[legs1[i]].add(df[legs1[i-1]], fill_value=0)\n",
    "                    #ddding the data to chart\n",
    "                    rthis = \\\n",
    "                    p.vbar(x = df['ChartValue1'].tolist(), width=0.5, bottom = df[legs1[i-1]], top=df[legs1[i]],\\\n",
    "                           color=colors[i])\n",
    "                    legendentry = (legs1[i], [rthis])\n",
    "                    r.append(rthis)\n",
    "                    legends.append(legendentry)\n",
    "                else:\n",
    "                    #adding first stack to chart\n",
    "                    rthis = p.vbar(x =  df['ChartValue1'].tolist(), width=0.5,  top=df[legs1[i]], color=colors[i],\\\n",
    "                                   legend=legs1[i], line_width=0)\n",
    "                    legendentry = (legs1[i], [rthis])\n",
    "                    r.append(rthis)\n",
    "                    legends.append(legendentry)\n",
    "                    r.append(rthis)\n",
    "            else:\n",
    "                bottomrow = df[legs1[len(legs1)-1]]\n",
    "                toprow = df[legs1[len(legs1)-1]]\n",
    "                rthis = p.vbar(x = df['ChartValue1'].tolist(), width=0.5, bottom = df[legs1[i-1]], \\\n",
    "                               top=df[legs1[i]], color=colors[i], legend=legs1[i], line_width=0)\n",
    "                r.append(rthis)\n",
    "        p.title.text = 'Distribution by ' + df['Variable'][0] + ' and ' + df['ChartCol2'][0] #Adding chart title\n",
    "    else:\n",
    "        #for chart without stacking\n",
    "        #summarize the data at category level - keep sorting intact  - required for buckets of numeric variable\n",
    "        df = df.groupby(['Variable', 'ChartValue1'], sort=False)['ChartValue3'].sum().reset_index().fillna(0)\n",
    "        #setting the X-axis range to Category Values in datafarme\n",
    "        p.x_range.factors = df['ChartValue1'].tolist()\n",
    "        #creating x-values list for plotting\n",
    "        xval = list(range(1, len(df)+1))\n",
    "        #adding barchart to figure\n",
    "        rthis = p.vbar(x =  df['ChartValue1'].tolist(), width=0.5,  top=df['ChartValue3'], color=defaultcolor, line_width=0)\n",
    "        r.append(rthis)\n",
    "        for i in range(1,maxcharts):\n",
    "            rthis = p.vbar(x = df['ChartValue1'].tolist(), width=0.5,  top=df['ChartValue3'], bottom=df['ChartValue3'],\\\n",
    "                           color=defaultcolor, line_width=0)\n",
    "            r.append(rthis)\n",
    "        p.title.text = 'Distribution by ' + df['Variable'][0] #Adding chart title\n",
    "    \n",
    "    #formatting chart elements\n",
    "    p.xgrid.grid_line_color = None #removing horizontal gridlines \n",
    "    p.ygrid.grid_line_color = None #removing vertical gridlines\n",
    "    \n",
    "    #if there are more than 4 categories on x-axis, rotate x-axis labels by 90 degree\n",
    "    if(len(df)>3):\n",
    "        p.xaxis.major_label_orientation = pi/2\n",
    "    p.xaxis.axis_label=df['Variable'][0] #adding x-axis title\n",
    "    p.yaxis.axis_label='Count of Records'#adding y-axis title\n",
    "    p.yaxis.minor_tick_line_color = None # removing y-axis minor ticklines\n",
    "    \n",
    "    #adding call out for missing values\n",
    "    #location for callout will change based on if there are stacks, i.e. there are legends that needs to be placed\n",
    "    citation = lb(x=70, y=340, x_units='screen', y_units='screen', \\\n",
    "                     text=\"\", text_font_size=\"10pt\",\\\n",
    "                     text_font_style= 'italic', text_color='red', render_mode='canvas')\n",
    "    if cntmiss>0:\n",
    "        if totcnt>1:\n",
    "            #place call out on left top for chart with legends - as legends will come on right top\n",
    "            citation = lb(x=70, y=340, x_units='screen', y_units='screen', \\\n",
    "                     text='Missing Values : ' + '{:,.0f}'.format(cntmiss), text_font_size=\"10pt\",\\\n",
    "                     text_font_style= 'italic', text_color='red', render_mode='canvas')\n",
    "        else:\n",
    "            #place call out on right top for chart without legends\n",
    "            citation = lb(x=400, y=340, x_units='screen', y_units='screen', \\\n",
    "                     text='Missing Values : ' + '{:,.0f}'.format(cntmiss), text_font_size=\"10pt\",\\\n",
    "                     text_font_style= 'italic', text_color='red', render_mode='canvas')\n",
    "    p.add_layout(citation)\n",
    "    plegend = Legend(items=legends, location='top_left')\n",
    "    p.add_layout(plegend)\n",
    "\n",
    "    return r , citation, plegend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to update existing Bar Chart with new data <a name = \"UpdateChart\"> </a>\n",
    "This method updates the only backend data for a chart, making transition smooth on change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to create a Bar Chart\n",
    "def updatebarchart(df): \n",
    "    \"\"\"Method to Update a bar chart\n",
    "    \n",
    "    Given exisitng bokeh and and Dataframe, if there are more than one class, \n",
    "    iteratively plot all the classes on top of each other (stacked). \n",
    "    Missing values are put as a Caveat - call out instead of plotting as a separate column\n",
    "    \"\"\"\n",
    "    \n",
    "    miss = df[df['ChartValue1']=='Missing']#Take out missing values in a separate dataframe\n",
    "    df = df[~(df['ChartValue1']=='Missing')]#Keep non-missing categories data\n",
    "    cntmiss = miss['ChartValue3'].sum() #get nuber of missing values for callout\n",
    "    totcnt = df['ChartValue2'].nunique() #number of unique stacking categories to be plotted\n",
    "    max1 = df.groupby(['ChartValue1'])['ChartValue3'].sum().reset_index()\n",
    "    ymax=max1['ChartValue3'].max()*1.2\n",
    "    print(ymax)\n",
    "    p.y_range=Range1d(0, ymax) # set the chart y-axis to be started at zero\n",
    "    legends = []\n",
    "    if totcnt>1:\n",
    "        # check if there are more than one stacking category, i.e. stacking needs to be done on chart\n",
    "        #Summarize data by the category and stacking values\n",
    "        #unstack the value to create wide data\n",
    "        #using groupby and unstack as order of the stacking and group needs to be retained\n",
    "        #pivot does not reatin the order \n",
    "        df = df.groupby(['Variable', 'ChartCol2', 'ChartValue1', 'ChartValue2'], \\\n",
    "                        sort=False)['ChartValue3'].sum().unstack('ChartValue2').reset_index().fillna(0)\n",
    "        #getting the names of stacking from column Names\n",
    "        legs1 = df.columns[3:]\n",
    "        #setting the X-axis range to Category Values in datafarme\n",
    "        p.x_range.factors = df['ChartValue1'].tolist()\n",
    "        #creating x-values list for plotting\n",
    "        xval = list(range(1, len(df)+1))\n",
    "        i1 = 0\n",
    "        for i in range(maxcharts):\n",
    "            if i<len(legs1):\n",
    "                #iteratively add all the stack bars on chart\n",
    "                if i>=1:\n",
    "                    #for any addtional stack (not the first stack), create the top value by adding the last value\n",
    "                    df[legs1[i]] = df[legs1[i]].add(df[legs1[i-1]], fill_value=0)\n",
    "                    #ddding the data to chart\n",
    "                    r[i].data_source.data = {'x': df['ChartValue1'].tolist(), 'top': df[legs1[i]].tolist(),\\\n",
    "                                            'bottom' : df[legs1[i-1]].tolist()}\n",
    "                    r[i].glyph.fill_color=colors[i]\n",
    "                    r[i].glyph.line_color=colors[i]\n",
    "                    r[i].glyph.line_width=0\n",
    "                    legendentry = (legs1[i], [r[i]])\n",
    "                    legends.append(legendentry)\n",
    "                else:\n",
    "                    #adding first stack to chart\n",
    "                    r[i].data_source.data = {'x': df['ChartValue1'].tolist(), 'top': df[legs1[i]].tolist()}\n",
    "                    r[i].glyph.fill_color=colors[i]\n",
    "                    r[i].glyph.line_color=colors[i]\n",
    "                    r[i].glyph.line_width=0\n",
    "                    legendentry = (legs1[i], [r[i]])\n",
    "                    legends.append(legendentry)\n",
    "                i1 = i\n",
    "            else:\n",
    "                bottomrow = df[legs1[len(legs1)-1]].tolist()\n",
    "                toprow = df[legs1[len(legs1)-1]].tolist()\n",
    "                r[i].data_source.data = {'x': df['ChartValue1'].tolist(), 'top': toprow, 'bottom':bottomrow}\n",
    "                r[i].glyph.fill_color=colors[i1]\n",
    "                r[i].glyph.line_color=colors[i1]\n",
    "                r[i].glyph.line_width=0\n",
    "        p.title.text = 'Distribution by ' + df['Variable'][0] + ' and ' + df['ChartCol2'][0] #Adding chart title\n",
    "    else:\n",
    "        #for chart without stacking\n",
    "        #summarize the data at category level - keep sorting intact  - required for buckets of numeric variable\n",
    "        df = df.groupby(['Variable', 'ChartValue1'], sort=False)['ChartValue3'].sum().reset_index().fillna(0)\n",
    "        #setting the X-axis range to Category Values in datafarme\n",
    "        p.x_range.factors = df['ChartValue1'].tolist()\n",
    "        #creating x-values list for plotting\n",
    "        xval = list(range(1, len(df)+1))\n",
    "        #adding barchart to figure\n",
    "        r[0].data_source.data = {'x': df['ChartValue1'].tolist(), 'top': df['ChartValue3'].tolist()}\n",
    "        r[0].glyph.fill_color=defaultcolor\n",
    "        r[0].glyph.line_color=defaultcolor\n",
    "        r[0].glyph.line_width=0\n",
    "        for i in range(1, maxcharts):\n",
    "            r[i].data_source.data = {'x': df['ChartValue1'].tolist(), 'top': df['ChartValue3'].tolist(), \\\n",
    "                                    'bottom' : df['ChartValue3'].tolist()}\n",
    "            r[i].glyph.fill_color=defaultcolor\n",
    "            r[i].glyph.line_color=defaultcolor\n",
    "            r[i].glyph.line_width=0\n",
    "        p.title.text = 'Distribution by ' + df['Variable'][0] #Adding chart title\n",
    "    \n",
    "    if(len(df)>3):\n",
    "        p.xaxis.major_label_orientation = pi/2\n",
    "    else:\n",
    "        p.xaxis.major_label_orientation = 0\n",
    "    p.xaxis.axis_label=df['Variable'][0] #adding x-axis title\n",
    "    \n",
    "    if cntmiss>0:\n",
    "        if totcnt>1:\n",
    "            #place call out on left top for chart with legends - as legends will come on right top\n",
    "            citation.text='Missing Values : ' + '{:,.0f}'.format(cntmiss)\n",
    "        else:\n",
    "            #place call out on right top for chart without legends\n",
    "            citation.text='Missing Values : ' + '{:,.0f}'.format(cntmiss)\n",
    "    else:\n",
    "        citation.text=''\n",
    "    plegend.items=legends\n",
    "    if len(legends)>2:\n",
    "        plegend.orientation = \"horizontal\"\n",
    "        plegend.location = 'top_left'\n",
    "    else:\n",
    "        plegend.orientation = \"vertical\"\n",
    "        plegend.location = 'top_right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to create empty bar chart **\n",
    "\n",
    "Used with variables with all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emptybarchart():\n",
    "    x = [\"Empty\"]\n",
    "    y = [0]\n",
    "    p.x_range.factors=x\n",
    "    citation.text = ''\n",
    "    plegend.items = []\n",
    "    for i in range(len(r)):\n",
    "        r[i].data_source.data = {'x': x, 'top': y, 'bottom' : y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Javascript to create download button**\n",
    "\n",
    "Used a dataframe and creates a CSV file on the fly and serve as download. This call back function is used to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scerror = ColumnDataSource(data=errorDF)\n",
    "\n",
    "callback = CustomJS(args=dict(source=scerror), code=\"\"\"\n",
    "var data = source.data;\n",
    "var filetext = 'Type of Data,Variable,Dimension,RuleID, origindex,Value\\\\n';\n",
    "\n",
    "for (i=0; i < data['Type of Data'].length; i++) {\n",
    "var currRow = [data['Type of Data'][i].toString(), data['Variable'][i].toString(),\n",
    "data['Dimension'][i].toString(), data['RuleID'][i].toString(), data['origIndex'][i].toString(),\n",
    " data['Value'][i].toString().concat('\\\\n')];\n",
    "var joined = currRow.join();\n",
    "filetext = filetext.concat(joined);\n",
    "}\n",
    "var filename = 'data.csv';\n",
    "var blob = new Blob([filetext], { type: 'text/csv;charset=utf-8;' });\n",
    "\n",
    "//addresses IE\n",
    "if (navigator.msSaveBlob) {\n",
    "navigator.msSaveBlob(blob, filename);\n",
    "}\n",
    "\n",
    "else {\n",
    "var link = document.createElement(\"a\");\n",
    "link = document.createElement('a')\n",
    "link.href = URL.createObjectURL(blob);\n",
    "link.download = filename\n",
    "link.target = \"_blank\";\n",
    "link.style.visibility = 'hidden';\n",
    "link.dispatchEvent(new MouseEvent('click'))\n",
    "}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the final page <a name=\"finalpg\"></a>\n",
    "\n",
    "In the code section below, all different components of visualizations are being created and laid-out in respective position on the webpage. Various functions are also being assigned to the widgets\n",
    "\n",
    "see [BarChart Method](#Barchart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Static Frame on top - Tool Title **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "## Top static frame to give tool title\n",
    "#######################################################################################\n",
    "\n",
    "#creating div for company logo\n",
    "imgdiv = Div(text= \"\")\n",
    "#creating div for page title\n",
    "titlediv = Div(text = \"<h1>Minimum Data Quality Tool</h1>\")\n",
    "pdiv = row(widgetbox(imgdiv, width=60), widgetbox(titlediv, width=540))#adding both the div in horizontal row\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable Level Plots and Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "## Variable Level plots and scores ####\n",
    "#######################################################################################\n",
    "#creating a div with description of variable. intially, the amount billed is plot, and the description is also for amount billed\n",
    "divdesc = Div(text=\"<h4> Description</h4>\" + mapping[mapping['Variable']=='Amount billed'].reset_index()['Description'][0],\\\n",
    "          width = 600, height=100)\n",
    "#getting data for Amount billed - for initial plotting\n",
    "tmpdf = chartDF[chartDF['Variable']=='Amount billed']\n",
    "p = figure(plot_width=600, plot_height=400, x_range = FactorRange(), name='plot',) #defining a figure object\n",
    "r, citation, plegend= makebarchart(p, tmpdf)#filling figure with Amount billed chart\n",
    "scoredfDimension.Score = scoredfDimension.Score.round(2) #Rounding-off the variable dimension-wise score - 2 digit rounding\n",
    "scoreOvDimension.Score = scoreOvDimension.Score.round(2) #Rounding-off the dimension-wise score - 2 digit rounding\n",
    "indf = scoredfDimension[scoredfDimension['Variable'] == 'Amount billed']#getting scores only for Amount-billed - initial plot\n",
    "insource = ColumnDataSource(indf) #defining source table for a Table at variable-dimension level\n",
    "intemplate = \"\"\"<span href=\"#\" data-toggle=\"tooltip\" title=\"<%= DescriptionToshow %>\"><%= value %></span>\"\"\"\n",
    "informater =  HTMLTemplateFormatter(template=intemplate) #Defining HTML Table  format\n",
    "#Defining columns for tables\n",
    "columns_in = [\n",
    "        TableColumn(field=\"Dimension\", title=\"Dimension\", formatter=informater, width=375),\n",
    "        TableColumn(field=\"Score\", title=\"Score\", width=200),\n",
    "    ]\n",
    "#create a data table widget for variable level dimension-wise score\n",
    "data_table_in = DataTable(source=insource, columns=columns_in, width=600, height=100, name='intable', row_headers=False)\n",
    "#Add the variable description, \n",
    "pane2 = column(widgetbox(divdesc, width=600, height=100), \\\n",
    "               p, widgetbox(data_table_in, width=600, height=100), name='pane2', width=400, height=600)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "## Overall Variables Scores Tables ####\n",
    "#######################################################################################\n",
    "scoredf.Score = scoredf.Score.round(2) #Rounding the scores to 2 digits\n",
    "scoretmp = scoredf[scoredf['Dimension']=='Overall']\n",
    "source = ColumnDataSource(scoretmp) #Creating column datasource for Table\n",
    "#Defining Template for the Table Values\n",
    "template=\"\"\"\n",
    "<div style=\"background:<%= \n",
    "    (function colorfromint(){\n",
    "        if(value > 0.5){\n",
    "            return(\"#D0FFD8\")}\n",
    "        else{return(\"#FFD7D0\")}\n",
    "        }()) %>; \n",
    "    color: black\"> \n",
    "<%= value %></div>\n",
    "\"\"\"\n",
    "formater =  HTMLTemplateFormatter(template=template) #Defining HTML Table  format\n",
    "\n",
    "#Defining columns for the table\n",
    "columns = [\n",
    "        TableColumn(field=\"Type of Data\", title=\"Type\", width = 75),\n",
    "        TableColumn(field=\"Variable\", title=\"Variable\", width=200),\n",
    "    TableColumn(field=\"Score\", title=\"Score\", formatter=formater, width = 50),\n",
    "    ]\n",
    "\n",
    "#Creating Table for visualization\n",
    "data_table = DataTable(source=source, columns=columns, width=325, height=550)\n",
    "\n",
    "menu = [(\"Overall\", \"Overall\"), (\"Completeness\", \"Completeness\"), (\"Validity\", \"Validity\"),\\\n",
    "        (\"Accuracy\", \"Accuracy\"), (\"Integrity\", \"Integrity\")]\n",
    "select = Select(value=\"Overall\", options=menu, width=325, height=40)\n",
    "selectwd = widgetbox(select, width=400, height=50)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "## Adding Variable level Visulaizations in place ####\n",
    "#######################################################################################\n",
    "\n",
    "#adding table to widgetbox for Visualization\n",
    "p2 = widgetbox(data_table, width=400, height=550)\n",
    "\n",
    "#adding both table and variable level visualization in a row\n",
    "pane = row(column(selectwd, p2, height=600, width=400), pane2, name = 'pane', width=800, height=600)\n",
    "\n",
    "#adding all detailed vizualization in a tab\n",
    "tab2 = Panel(child=pane, title=\"Detail\", name='tab2', width=800, height=600)\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "## Overall Data Score and Overall Dimension wise Table ####\n",
    "#######################################################################################\n",
    "\n",
    "OverAll = ColumnDataSource(scoreOvDimension)\n",
    "overcolumns = [\n",
    "        TableColumn(field=\"Dimension\", title=\"Dimension\"),\n",
    "    TableColumn(field=\"Score\", title=\"Score\", formatter=formater),\n",
    "    ]\n",
    "\n",
    "Overtable = DataTable(source=OverAll, columns=overcolumns, width=500, height=250, row_headers=False)\n",
    "Overdiv = Div(text = '<div style=\"background-color:lightgrey;padding: 0; margin: 0\">'+\\\n",
    "              '<br><h1 align=\"center\"> OverAll Score</h1> <h2 align=\"center\">'+\\\n",
    "              str(oscore.round(2)) + '</h2><br></div>', \\\n",
    "              width=275, height=200)\n",
    "r1 = row(widgetbox(Overdiv, width = 300, height=200), widgetbox(Overtable, width = 500, height=250), width=800, height=250)\n",
    "\n",
    "downbutton = Button(label='Download Error Data', button_type='success', callback=callback, height=50, width=200)\n",
    "\n",
    "Dimdiv = Div(text = \" \", \\\n",
    "                     width= 800, height=200, name='Dimdiv')\n",
    "#######################################################################################\n",
    "\n",
    "#######################################################################################\n",
    "## Creating Tab for overall Data Level Information ####\n",
    "#######################################################################################\n",
    "\n",
    "wddim=widgetbox(Dimdiv, width= 800, height=350, name='wddim')\n",
    "wddown=widgetbox(downbutton, width= 800, height=75, name='wddown')\n",
    "c1 = column(r1,wddown,wddim, width=800, height=600, name='c1')\n",
    "tab1 = Panel(child=c1, title=\"Summary\", width=800, height=600, name='tab1')\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "#######################################################################################\n",
    "## Putting Tabs and Top Pane in visualization ####\n",
    "#######################################################################################\n",
    "\n",
    "tabs = Tabs(tabs=[ tab1, tab2 ], name = 'MainLayout')\n",
    "hdr = column(pdiv, tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to update charts on click on Table**<a name=\"Tableupdate\"></a>\n",
    "\n",
    "The code below defines a function which takes the value of variable from variable level Table (on click), and refreshes the chart and detailed variable-dimension wise table for the value selected on detailed table. Function just refreshed the backend data in the table and charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_to_call(attr, old, new):\n",
    "    val = data_table.source.data['Variable'][new['1d']['indices'][0]]\n",
    "    print(val)\n",
    "    tmpdf = chartDF[chartDF['Variable']==val]\n",
    "    divdesc.text=\"<h1> Description</h1>\" + mapping[mapping['Variable']==val].reset_index()['Description'][0]\n",
    "    if(len(tmpdf)>0):\n",
    "        print(len(tmpdf))\n",
    "        updatebarchart(tmpdf)\n",
    "        indf = scoredfDimension[scoredfDimension['Variable'] == val]\n",
    "        insource = ColumnDataSource(indf) \n",
    "        intemplate = \"\"\"<span href=\"#\" data-toggle=\"tooltip\" title=\"<%= DescriptionToshow %>\"><%= value %></span>\"\"\"\n",
    "        informater =  HTMLTemplateFormatter(template=intemplate) #Defining HTML Table  format\n",
    "        columns_in = [\n",
    "                TableColumn(field=\"Dimension\", title=\"Dimension\", width=375, formatter=informater),\n",
    "                TableColumn(field=\"Score\", title=\"Score\", width=200),\n",
    "            ]\n",
    "        data_table_in.source =insource\n",
    "        data_table_in.columns =columns_in\n",
    "    else:\n",
    "        emptybarchart()\n",
    "        indf = scoredfDimension[scoredfDimension['Variable'] == 'Thisisnotavalue']\n",
    "        insource = ColumnDataSource(indf)\n",
    "        intemplate = \"\"\"<span href=\"#\" data-toggle=\"tooltip\" title=\"<%= DescriptionToshow %>\"><%= value %></span>\"\"\"\n",
    "        informater =  HTMLTemplateFormatter(template=intemplate) #Defining HTML Table  format\n",
    "        columns_in = [\n",
    "                TableColumn(field=\"Dimension\", title=\"Dimension\", width=375, formatter=informater),\n",
    "                TableColumn(field=\"Score\", title=\"Score\", width=200),\n",
    "            ]\n",
    "        data_table_in.source =insource\n",
    "        data_table_in.columns =columns_in\n",
    "        divdesc.text=\"\"\"<h1> Data Missing for \"\"\" + val +\\\n",
    "        \"\"\"</h1> Please ignore table below\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to update the description of dimension**\n",
    "\n",
    "Method just updates the text of the div with dimension description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def function_outerTable(attr, old, new):\n",
    "    val = scoreOvDimension['Dimension'][new['1d']['indices'][0]]\n",
    "    if val=='Completeness':\n",
    "        Dimdiv.text = \"<br><h1>Completeness</h1>\"+\\\n",
    "                     \"<h3 align=\"\"left\"\"> Definition</h3>It is defined as expected comprehensiveness.<br>\"+\\\n",
    "                     \"<h3>Calculation Method</h3>Calculate fill rates for each variable and decide a minimum % fill\"+\\\n",
    "                    \" rate for a variable to be complete. For mandatory data items, \"+\\\n",
    "                     \"for example – claim ID, Policy start date etc., 100% completeness is required A \\\n",
    "                     measure of the blank (null or empty string) values or the presence of non-blank values.\"\n",
    "    elif val=='Accuracy':\n",
    "        Dimdiv.text = \"<br><h1>Accuracy</h1>\"+\\\n",
    "                     \"<h3 align=\"\"left\"\"> Definition</h3>It is defined as is the degree to which data correctly reflects \"+\\\n",
    "                     \"the real world object or an event being described. For example –<br>\"+\\\n",
    "                     \"<ol><li>Age of insured is between 5 and 95</li><li>Policy start date is after 1/1/1999</li>\" +\\\n",
    "                     \"<li>Address of the employee has no spelling mistakes and is the real address</li></ol> \\\n",
    "                     <h3>Calculation Method</h3>Calculate values, for relevant variables, which conform to the range or \\\n",
    "                     correct spelling measure.\"        \n",
    "    elif val=='Uniqueness':\n",
    "        Dimdiv.text = \"<br><h1>Uniqueness</h1>\"+\\\n",
    "                     \"<h3 align=\"\"left\"\"> Definition</h3>It is defined as a measure of unwanted duplication\" +\\\n",
    "                     \"existing within or across \"+\\\n",
    "                     \"systems for a particular field, record, or data set. For example – \"+\\\n",
    "                     \"One policyID is exists twice in policy level data \\\n",
    "                     <h3>Calculation Method</h3>Calculate if there are any observations which do not have any \\\n",
    "                     differentiation in values\"\n",
    "        \n",
    "    elif val=='Integrity':\n",
    "        Dimdiv.text = \"<br><h1>Integrity</h1>\"+\\\n",
    "                     \"<h3 align=\"\"left\"\"> Definition</h3>It is defined as a measure of variable values \"+\\\n",
    "                     \"corresponding to particular set of rules, natural or defined by business. \"+\\\n",
    "                     \"For example, date of birth of insured is greater than policy start date \"+\\\n",
    "                     \"or claim registration date is smaller than claim payment date. \\\n",
    "                     <h3>Calculation Method</h3>Comparison of data format with metadata or data documentation\"\n",
    "        \n",
    "    elif val=='Validity':\n",
    "        Dimdiv.text = \"<br><h1>Validity</h1>\"+\\\n",
    "                     \"<h3 align=\"\"left\"\"> Definition</h3>It is defined such that the data is following a set of \"+\\\n",
    "                     \"standard data definitions like data type, size and format. For example, \"+\\\n",
    "                     \"date of birth of customer is in the format “mm/dd/yyyy”.\"+\\\n",
    "                     \"<h3>Calculation Method</h3>Comparison of variable values which should conform to defined rules\"\n",
    "    else:\n",
    "        Dimdiv.text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to update the variable level table basis dropdown values**\n",
    "\n",
    "Method update the backend data for the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeTable(attr, old, new):\n",
    "    val = select.value\n",
    "    scoretmp = scoredf[scoredf['Dimension']==val]\n",
    "    sourcenew = ColumnDataSource(scoretmp) #Creating column datasource for Table\n",
    "    source.data = sourcenew.data\n",
    "    #source.on_change('selected', function_to_call)\n",
    "    #Defining Template for the Table Values\n",
    "    template=\"\"\"\n",
    "    <div style=\"background:<%= \n",
    "        (function colorfromint(){\n",
    "            if(value > 0.5){\n",
    "                return(\"#D0FFD8\")}\n",
    "            else{return(\"#FFD7D0\")}\n",
    "            }()) %>; \n",
    "        color: black\"> \n",
    "    <%= value %></div>\n",
    "    \"\"\"\n",
    "    formater =  HTMLTemplateFormatter(template=template) #Defining HTML Table  format\n",
    "    \n",
    "    #Defining columns for the table\n",
    "    columns = [\n",
    "            TableColumn(field=\"Type of Data\", title=\"Type\", width = 75),\n",
    "            TableColumn(field=\"Variable\", title=\"Variable\", width=200),\n",
    "            TableColumn(field=\"Score\", title=\"Score\", formatter=formater, width = 50),\n",
    "        ]\n",
    "    data_table.source.data =sourcenew.data\n",
    "    data_table.columns=columns\n",
    "    data_table_force_change = CustomJS(args=dict(source=source), code=\"\"\"source.change.emit()\"\"\")\n",
    "    source.js_on_change('data', data_table_force_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source.on_change('selected', function_to_call)\n",
    "OverAll.on_change('selected', function_outerTable)\n",
    "select.on_change('value', changeTable)\n",
    "curdoc().add_root(hdr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
